1:"$Sreact.fragment"
2:I[16501,["771","static/chunks/771-6df7383af9cb22cb.js","732","static/chunks/732-1930f1f7bd6746a5.js","13","static/chunks/13-f12e2db1f59fda05.js","355","static/chunks/355-f98f115f8c6f56c6.js","177","static/chunks/app/layout-be7f58cf6f9b9f21.js"],"Provider"]
3:I[9766,[],""]
4:I[98924,[],""]
5:I[58923,["771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","335","static/chunks/app/%5Bslug%5D/error-b2784470a4722ef4.js"],"default"]
6:I[52619,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],""]
7:I[54921,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],"Button"]
9:I[24431,[],"OutletBoundary"]
b:I[15278,[],"AsyncMetadataOutlet"]
d:I[24431,[],"ViewportBoundary"]
f:I[24431,[],"MetadataBoundary"]
10:"$Sreact.suspense"
12:I[57150,[],""]
:HL["/kouchouAI-reports/_next/static/css/a63443551c7d7d9f.css","style"]
0:{"P":null,"b":"3b2kFua5oe-8CgSo307Vs","p":"/kouchouAI-reports","c":["","d3325284-0e4d-4c51-959e-241520565899",""],"i":false,"f":[[["",{"children":[["slug","d3325284-0e4d-4c51-959e-241520565899","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/kouchouAI-reports/_next/static/css/a63443551c7d7d9f.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"suppressHydrationWarning":true,"lang":"ja","children":[["$","head",null,{"children":[["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"href":"https://fonts.googleapis.com/css2?family=BIZ+UDPGothic&display=swap","rel":"stylesheet"}],["$","link",null,{"rel":"icon","href":"/kouchouAI-reports/meta/icon.png","sizes":"any"}],false]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}]]}],{"children":[["slug","d3325284-0e4d-4c51-959e-241520565899","d"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$5","errorStyles":[],"errorScripts":[],"template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","p",null,{"children":"ページが見つかりませんでした"}],["$","$L6",null,{"href":"/","children":["$","$L7",null,{"children":"トップに戻る"}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L8",null,["$","$L9",null,{"children":["$La",["$","$Lb",null,{"promise":"$@c"}]]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,[["$","$Ld",null,{"children":"$Le"}],null],["$","$Lf",null,{"children":["$","div",null,{"hidden":true,"children":["$","$10",null,{"fallback":null,"children":"$L11"}]}]}]]}],false]],"m":"$undefined","G":["$12",[]],"s":false,"S":true}
e:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:null
13:I[67733,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],"Header"]
14:I[99347,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],"Box"]
15:I[55756,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],"Heading"]
16:I[48409,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],"Text"]
17:I[92091,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],"Icon"]
18:I[6026,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],"ClientContainer"]
19:T194c,import concurrent.futures
import json
import logging
import os
import re

import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai
from services.parse_json_list import parse_extraction_response
from utils import update_progress

COMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r",\s*(\])")


class ExtractionResponse(BaseModel):
    extractedOpinionList: list[str] = Field(..., description="抽出した意見のリスト")


def _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:
    if not all(property in comments.columns for property in property_columns):
        raise ValueError(f"Properties {property_columns} not found in comments. Columns are {comments.columns}")


def extraction(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/args.csv"
    model = config["extraction"]["model"]
    prompt = config["extraction"]["prompt"]
    workers = config["extraction"]["workers"]
    limit = config["extraction"]["limit"]
    property_columns = config["extraction"]["properties"]

    if "provider" not in config:
        raise RuntimeError("provider is not set")
    provider = config["provider"]

    # カラム名だけを読み込み、必要なカラムが含まれているか確認する
    comments = pd.read_csv(f"inputs/{config['input']}.csv", nrows=0)
    _validate_property_columns(property_columns, comments)
    # エラーが出なかった場合、すべての行を読み込む
    comments = pd.read_csv(
        f"inputs/{config['input']}.csv", usecols=["comment-id", "comment-body"] + config["extraction"]["properties"]
    )
    comment_ids = (comments["comment-id"].values)[:limit]
    comments.set_index("comment-id", inplace=True)
    results = pd.DataFrame()
    update_progress(config, total=len(comment_ids))

    argument_map = {}
    relation_rows = []

    for i in tqdm(range(0, len(comment_ids), workers)):
        batch = comment_ids[i : i + workers]
        batch_inputs = [comments.loc[id]["comment-body"] for id in batch]
        batch_results = extract_batch(
            batch_inputs, prompt, model, workers, provider, config.get("local_llm_address"), config
        )

        for comment_id, extracted_args in zip(batch, batch_results, strict=False):
            for j, arg in enumerate(extracted_args):
                if arg not in argument_map:
                    # argumentテーブルに追加
                    arg_id = f"A{comment_id}_{j}"
                    argument = arg
                    argument_map[arg] = {
                        "arg-id": arg_id,
                        "argument": argument,
                    }
                else:
                    arg_id = argument_map[arg]["arg-id"]

                # relationテーブルにcommentとargの関係を追加
                relation_row = {
                    "arg-id": arg_id,
                    "comment-id": comment_id,
                }
                relation_rows.append(relation_row)

        update_progress(config, incr=len(batch))

    # DataFrame化
    results = pd.DataFrame(argument_map.values())
    relation_df = pd.DataFrame(relation_rows)

    if results.empty:
        raise RuntimeError("result is empty, maybe bad prompt")

    results.to_csv(path, index=False)
    # comment-idとarg-idの関係を保存
    relation_df.to_csv(f"outputs/{dataset}/relations.csv", index=False)


logging.basicConfig(level=logging.DEBUG)


def extract_batch(batch, prompt, model, workers, provider="openai", local_llm_address=None, config=None):
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures_with_index = [
            (i, executor.submit(extract_arguments, input, prompt, model, provider, local_llm_address))
            for i, input in enumerate(batch)
        ]

        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)
        results = [[] for _ in range(len(batch))]
        total_token_input = 0
        total_token_output = 0
        total_token_usage = 0

        for _, future in futures_with_index:
            if future in not_done and not future.cancelled():
                future.cancel()

        for i, future in futures_with_index:
            if future in done:
                try:
                    result = future.result()
                    if isinstance(result, tuple) and len(result) == 4:
                        items, token_input, token_output, token_total = result
                        results[i] = items
                        total_token_input += token_input
                        total_token_output += token_output
                        total_token_usage += token_total
                    else:
                        results[i] = result
                except Exception as e:
                    logging.error(f"Task {future} failed with error: {e}")
                    results[i] = []

        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + total_token_usage
            config["token_usage_input"] = config.get("token_usage_input", 0) + total_token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + total_token_output
            print(
                f"Extraction batch: input={total_token_input}, output={total_token_output}, total={total_token_usage} tokens"
            )

        return results


def extract_arguments(input, prompt, model, provider="openai", local_llm_address=None):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            is_json=False,
            json_schema=ExtractionResponse,
            provider=provider,
            local_llm_address=local_llm_address,
            user_api_key=os.getenv("USER_API_KEY"),
        )
        items = parse_extraction_response(response)
        items = list(filter(None, items))  # omit empty strings
        return items, token_input, token_output, token_total
    except json.decoder.JSONDecodeError as e:
        print("JSON error:", e)
        print("Input was:", input)
        print("Response was:", response)
        print("Silently giving up on trying to generate valid list.")
        return []
1a:T1149,"""Cluster the arguments using UMAP + HDBSCAN and GPT-4."""

from importlib import import_module

import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans


def hierarchical_clustering(config):
    UMAP = import_module("umap").UMAP

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_clusters.csv"
    arguments_df = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings_df = pd.read_pickle(f"outputs/{dataset}/embeddings.pkl")
    embeddings_array = np.asarray(embeddings_df["embedding"].values.tolist())
    cluster_nums = config["hierarchical_clustering"]["cluster_nums"]

    n_samples = embeddings_array.shape[0]
    # デフォルト設定は15
    default_n_neighbors = 15

    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる
    if n_samples <= default_n_neighbors:
        n_neighbors = max(2, n_samples - 1)  # 最低2以上
    else:
        n_neighbors = default_n_neighbors

    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)
    # TODO 詳細エラーメッセージを加える
    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因
    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
    umap_embeds = umap_model.fit_transform(embeddings_array)

    cluster_results = hierarchical_clustering_embeddings(
        umap_embeds=umap_embeds,
        cluster_nums=cluster_nums,
    )
    result_df = pd.DataFrame(
        {
            "arg-id": arguments_df["arg-id"],
            "argument": arguments_df["argument"],
            "x": umap_embeds[:, 0],
            "y": umap_embeds[:, 1],
        }
    )

    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):
        result_df[f"cluster-level-{cluster_level}-id"] = [f"{cluster_level}_{label}" for label in final_labels]

    result_df.to_csv(path, index=False)


def generate_cluster_count_list(min_clusters: int, max_clusters: int):
    cluster_counts = []
    current = min_clusters
    cluster_counts.append(current)

    if min_clusters == max_clusters:
        return cluster_counts

    while True:
        next_double = current * 2
        next_triple = current * 3

        if next_double >= max_clusters:
            if cluster_counts[-1] != max_clusters:
                cluster_counts.append(max_clusters)
            break

        # 次の倍はまだ max_clusters に収まるが、3倍だと超える
        # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ
        if next_triple > max_clusters:
            cluster_counts.append(max_clusters)
            break

        cluster_counts.append(next_double)
        current = next_double

    return cluster_counts


def merge_clusters_with_hierarchy(
    cluster_centers: np.ndarray,
    kmeans_labels: np.ndarray,
    umap_array: np.ndarray,
    n_cluster_cut: int,
):
    Z = sch.linkage(cluster_centers, method="ward")
    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion="maxclust")

    n_samples = umap_array.shape[0]
    final_labels = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        original_label = kmeans_labels[i]
        final_labels[i] = cluster_labels_merged[original_label]

    return final_labels


def hierarchical_clustering_embeddings(
    umap_embeds,
    cluster_nums,
):
    # 最大分割数でクラスタリングを実施
    print("start initial clustering")
    initial_cluster_num = cluster_nums[-1]
    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)
    kmeans_model.fit(umap_embeds)
    print("end initial clustering")

    results = {}
    print("start hierarchical clustering")
    cluster_nums.sort()
    print(cluster_nums)
    for n_cluster_cut in cluster_nums[:-1]:
        print("n_cluster_cut: ", n_cluster_cut)
        final_labels = merge_clusters_with_hierarchy(
            cluster_centers=kmeans_model.cluster_centers_,
            kmeans_labels=kmeans_model.labels_,
            umap_array=umap_embeds,
            n_cluster_cut=n_cluster_cut,
        )
        results[n_cluster_cut] = final_labels

    results[initial_cluster_num] = kmeans_model.labels_
    print("end hierarchical clustering")

    return results
1b:T1c36,import json
import os
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import TypedDict

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class LabellingResult(TypedDict):
    """各クラスタのラベリング結果を表す型"""

    cluster_id: str  # クラスタのID
    label: str  # クラスタのラベル名
    description: str  # クラスタの説明文


def hierarchical_initial_labelling(config: dict) -> None:
    """階層的クラスタリングの初期ラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_initial_labelling: 初期ラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_initial_labels.csv"
    clusters_argument_df = pd.read_csv(f"outputs/{dataset}/hierarchical_clusters.csv")

    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith("cluster-level-")]
    initial_cluster_id_column = cluster_id_columns[-1]
    sampling_num = config["hierarchical_initial_labelling"]["sampling_num"]
    initial_labelling_prompt = config["hierarchical_initial_labelling"]["prompt"]
    model = config["hierarchical_initial_labelling"]["model"]
    workers = config["hierarchical_initial_labelling"]["workers"]

    # トークン使用量を追跡するための変数を初期化
    config["total_token_usage"] = config.get("total_token_usage", 0)

    initial_label_df = initial_labelling(
        initial_labelling_prompt,
        clusters_argument_df,
        sampling_num,
        model,
        workers,
        config["provider"],
        config.get("local_llm_address"),
        config,  # configを渡して、トークン使用量を累積できるようにする
    )
    print("start initial labelling")
    initial_clusters_argument_df = clusters_argument_df.merge(
        initial_label_df,
        left_on=initial_cluster_id_column,
        right_on="cluster_id",
        how="left",
    ).rename(
        columns={
            "label": f"{initial_cluster_id_column.replace('-id', '')}-label",
            "description": f"{initial_cluster_id_column.replace('-id', '')}-description",
        }
    )
    print("end initial labelling")
    initial_clusters_argument_df.to_csv(path, index=False)


def initial_labelling(
    prompt: str,
    clusters_df: pd.DataFrame,
    sampling_num: int,
    model: str,
    workers: int,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> pd.DataFrame:
    """各クラスタに対して初期ラベリングを実行する

    Args:
        prompt: LLMへのプロンプト
        clusters_df: クラスタリング結果のDataFrame
        sampling_num: 各クラスタからサンプリングする意見の数
        model: 使用するLLMモデル名
        workers: 並列処理のワーカー数
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        各クラスタのラベリング結果を含むDataFrame
    """
    cluster_columns = [col for col in clusters_df.columns if col.startswith("cluster-level-")]
    initial_cluster_column = cluster_columns[-1]
    cluster_ids = clusters_df[initial_cluster_column].unique()
    process_func = partial(
        process_initial_labelling,
        df=clusters_df,
        prompt=prompt,
        sampling_num=sampling_num,
        target_column=initial_cluster_column,
        model=model,
        provider=provider,
        local_llm_address=local_llm_address,
        config=config,  # configを渡す
    )
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_func, cluster_ids))
    return pd.DataFrame(results)


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_initial_labelling(
    cluster_id: str,
    df: pd.DataFrame,
    prompt: str,
    sampling_num: int,
    target_column: str,
    model: str,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> LabellingResult:
    """個別のクラスタに対してラベリングを実行する

    Args:
        cluster_id: 処理対象のクラスタID
        df: クラスタリング結果のDataFrame
        prompt: LLMへのプロンプト
        sampling_num: サンプリングする意見の数
        target_column: クラスタIDが格納されている列名
        model: 使用するLLMモデル名
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        クラスタのラベリング結果
    """
    cluster_data = df[df[target_column] == cluster_id]
    sampling_num = min(sampling_num, len(cluster_data))
    cluster = cluster_data.sample(sampling_num)
    input = "\n".join(cluster["argument"].values)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            provider=provider,
            json_schema=LabellingFromat,
            local_llm_address=local_llm_address,
            user_api_key=os.getenv("USER_API_KEY"),
        )

        # トークン使用量を累積（configが渡されている場合）
        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
            config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + token_output

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return LabellingResult(
            cluster_id=cluster_id,
            label=response_json.get("label", "エラーでラベル名が取得できませんでした"),
            description=response_json.get("description", "エラーで解説が取得できませんでした"),
        )
    except Exception as e:
        print(e)
        return LabellingResult(
            cluster_id=cluster_id,
            label="エラーでラベル名が取得できませんでした",
            description="エラーで解説が取得できませんでした",
        )
1c:T337b,import json
import os
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from functools import partial

import numpy as np
import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai


@dataclass
class ClusterColumns:
    """同一階層のクラスター関連のカラム名を管理するクラス"""

    id: str
    label: str
    description: str

    @classmethod
    def from_id_column(cls, id_column: str) -> "ClusterColumns":
        """ID列名から関連するカラム名を生成"""
        return cls(
            id=id_column,
            label=id_column.replace("-id", "-label"),
            description=id_column.replace("-id", "-description"),
        )


@dataclass
class ClusterValues:
    """対象クラスタのlabel/descriptionを管理するクラス"""

    label: str
    description: str

    def to_prompt_text(self) -> str:
        return f"- {self.label}: {self.description}"


def hierarchical_merge_labelling(config: dict) -> None:
    """階層的クラスタリングの結果に対してマージラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_merge_labelling: マージラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    merge_path = f"outputs/{dataset}/hierarchical_merge_labels.csv"
    clusters_df = pd.read_csv(f"outputs/{dataset}/hierarchical_initial_labels.csv")

    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)
    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成
    merge_result_df = merge_labelling(
        clusters_df=clusters_df,
        cluster_id_columns=sorted(cluster_id_columns, reverse=True),
        config=config,
    )
    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成
    melted_df = melt_cluster_data(merge_result_df)
    # 上記のdfに親子関係を追加
    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)
    melted_df = melted_df.merge(parent_child_df, on=["level", "id"], how="left")
    density_df = calculate_cluster_density(melted_df, config)
    density_df.to_csv(merge_path, index=False)


def _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):
    """クラスタ間の親子関係をマッピングする

    Args:
        df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト

    Returns:
        親子関係のマッピング情報を含むDataFrame
    """
    results = []
    top_cluster_column = cluster_id_columns[0]
    top_cluster_values = df[top_cluster_column].unique()
    for c in top_cluster_values:
        results.append(
            {
                "level": 1,
                "id": c,
                "parent": "0",  # aggregationで追加する全体クラスタのid
            }
        )

    for idx in range(len(cluster_id_columns) - 1):
        current_column = cluster_id_columns[idx]
        children_column = cluster_id_columns[idx + 1]
        current_level = current_column.replace("-id", "").replace("cluster-level-", "")
        # 現在のレベルのクラスタid
        current_cluster_values = df[current_column].unique()
        for current_id in current_cluster_values:
            children_ids = df.loc[df[current_column] == current_id, children_column].unique()
            for child_id in children_ids:
                results.append(
                    {
                        "level": int(current_level) + 1,
                        "id": child_id,
                        "parent": current_id,
                    }
                )
    return pd.DataFrame(results)


def _filter_id_columns(columns: list[str]) -> list[str]:
    """クラスタIDのカラム名をフィルタリングする

    Args:
        columns: 全カラム名のリスト

    Returns:
        クラスタIDのカラム名のリスト
    """
    return [col for col in columns if col.startswith("cluster-level-") and col.endswith("-id")]


def melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:
    """クラスタデータを行形式に変換する

    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。
    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。

    Args:
        df: クラスタリング結果のDataFrame

    Returns:
        行形式に変換されたDataFrame
    """
    id_columns: list[str] = _filter_id_columns(df.columns)
    levels: set[int] = {int(col.replace("cluster-level-", "").replace("-id", "")) for col in id_columns}
    all_rows: list[dict] = []

    # levelごとに各クラスタの出現件数を集計・縦持ちにする
    for level in levels:
        cluster_columns = ClusterColumns.from_id_column(f"cluster-level-{level}-id")
        # クラスタidごとの件数集計
        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name="value")

        level_unique_val_df = df[
            [cluster_columns.id, cluster_columns.label, cluster_columns.description]
        ].drop_duplicates()
        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how="left")
        level_unique_vals = [
            {
                "level": level,
                "id": row[cluster_columns.id],
                "label": row[cluster_columns.label],
                "description": row[cluster_columns.description],
                "value": row["value"],
            }
            for _, row in level_unique_val_df.iterrows()
        ]
        all_rows.extend(level_unique_vals)
    return pd.DataFrame(all_rows)


def merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:
    """階層的なクラスタのマージラベリングを実行する

    Args:
        clusters_df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含むDataFrame
    """
    for idx in tqdm(range(len(cluster_id_columns) - 1)):
        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])
        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])

        process_fn = partial(
            process_merge_labelling,
            result_df=clusters_df,
            current_columns=current_columns,
            previous_columns=previous_columns,
            config=config,
        )

        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())
        with ThreadPoolExecutor(max_workers=config["hierarchical_merge_labelling"]["workers"]) as executor:
            responses = list(
                tqdm(
                    executor.map(process_fn, current_cluster_ids),
                    total=len(current_cluster_ids),
                )
            )

        current_result_df = pd.DataFrame(responses)
        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])
    return clusters_df


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_merge_labelling(
    target_cluster_id: str,
    result_df: pd.DataFrame,
    current_columns: ClusterColumns,
    previous_columns: ClusterColumns,
    config,
):
    """個別のクラスタに対してマージラベリングを実行する

    Args:
        target_cluster_id: 処理対象のクラスタID
        result_df: クラスタリング結果のDataFrame
        current_columns: 現在のレベルのカラム情報
        previous_columns: 前のレベルのカラム情報
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含む辞書
    """

    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:
        """前のレベルのクラスタ情報を取得する"""
        previous_records = df[df[current_columns.id] == target_cluster_id][
            [previous_columns.label, previous_columns.description]
        ].drop_duplicates()
        previous_values = [
            ClusterValues(
                label=row[previous_columns.label],
                description=row[previous_columns.description],
            )
            for _, row in previous_records.iterrows()
        ]
        return previous_values

    previous_values = filter_previous_values(result_df, previous_columns)
    if len(previous_values) == 1:
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: previous_values[0].label,
            current_columns.description: previous_values[0].description,
        }
    elif len(previous_values) == 0:
        raise ValueError(f"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。")

    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]
    sampling_num = min(
        config["hierarchical_merge_labelling"]["sampling_num"],
        len(current_cluster_data),
    )
    sampled_data = current_cluster_data.sample(sampling_num)
    sampled_argument_text = "\n".join(sampled_data["argument"].values)
    cluster_text = "\n".join([value.to_prompt_text() for value in previous_values])
    messages = [
        {"role": "system", "content": config["hierarchical_merge_labelling"]["prompt"]},
        {
            "role": "user",
            "content": "クラスタラベル\n" + cluster_text + "\n" + "クラスタの意見\n" + sampled_argument_text,
        },
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=config["hierarchical_merge_labelling"]["model"],
            json_schema=LabellingFromat,
            provider=config["provider"],
            local_llm_address=config.get("local_llm_address"),
            user_api_key=os.getenv("USER_API_KEY"),
        )

        config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
        config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
        config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
        print(f"Merge labelling: input={token_input}, output={token_output}, total={token_total} tokens")

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: response_json.get("label", "エラーでラベル名が取得できませんでした"),
            current_columns.description: response_json.get("description", "エラーで解説が取得できませんでした"),
        }
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: "エラーでラベル名が取得できませんでした",
            current_columns.description: "エラーで解説が取得できませんでした",
        }


def calculate_cluster_density(melted_df: pd.DataFrame, config: dict):
    """クラスタ内の密度計算"""
    hierarchical_cluster_df = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")

    densities = []
    for level, c_id in zip(melted_df["level"], melted_df["id"], strict=False):
        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f"cluster-level-{level}-id"] == c_id][
            ["x", "y"]
        ].values
        density = calculate_density(cluster_embeds)
        densities.append(density)

    # 密度のランクを計算
    melted_df["density"] = densities
    melted_df["density_rank"] = melted_df.groupby("level")["density"].rank(ascending=False, method="first")
    melted_df["density_rank_percentile"] = melted_df.groupby("level")["density_rank"].transform(lambda x: x / len(x))
    return melted_df


def calculate_density(embeds: np.ndarray):
    """平均距離に基づいて密度を計算"""
    center = np.mean(embeds, axis=0)
    distances = np.linalg.norm(embeds - center, axis=1)
    avg_distance = np.mean(distances)
    density = 1 / (avg_distance + 1e-10)
    return density
1d:Ta82,"""Create summaries for the clusters."""

import json
import os
import re

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class OverviewResponse(BaseModel):
    summary: str = Field(..., description="クラスターの全体的な要約")


def hierarchical_overview(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_overview.txt"

    hierarchical_label_df = pd.read_csv(f"outputs/{dataset}/hierarchical_merge_labels.csv")

    prompt = config["hierarchical_overview"]["prompt"]
    model = config["hierarchical_overview"]["model"]

    # TODO: level1で固定にしているが、設定で変えられるようにする
    target_level = 1
    target_records = hierarchical_label_df[hierarchical_label_df["level"] == target_level]
    ids = target_records["id"].to_list()
    labels = target_records["label"].to_list()
    descriptions = target_records["description"].to_list()
    target_records.set_index("id", inplace=True)

    input_text = ""
    for i, _ in enumerate(ids):
        input_text += f"# Cluster {i}/{len(ids)}: {labels[i]}\n\n"
        input_text += descriptions[i] + "\n\n"

    messages = [{"role": "system", "content": prompt}, {"role": "user", "content": input_text}]
    response_text, token_input, token_output, token_total = request_to_chat_ai(
        messages=messages,
        model=model,
        provider=config["provider"],
        local_llm_address=config.get("local_llm_address"),
        user_api_key=os.getenv("USER_API_KEY"),
        json_schema=OverviewResponse,
    )

    # トークン使用量を累積
    config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
    config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
    config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
    print(f"Hierarchical overview: input={token_input}, output={token_output}, total={token_total} tokens")

    try:
        # structured outputとしてパースできるなら処理する
        if isinstance(response_text, dict):
            parsed_response = response_text
        else:
            parsed_response = json.loads(response_text)

        with open(path, "w") as file:
            file.write(parsed_response["summary"])

    except Exception:
        # thinkタグが出力されるReasoningモデル用に、thinkタグを除去する
        thinking_removed = re.sub(
            r"<think\b[^>]*>.*?</think>",
            "",
            response_text,
            flags=re.DOTALL,
        )

        with open(path, "w") as file:
            file.write(thinking_removed)
1e:T3f38,"""Generate a convenient JSON output file."""

import json
from collections import defaultdict
from pathlib import Path
from typing import Any, TypedDict

import numpy as np
import pandas as pd

ROOT_DIR = Path(__file__).parent.parent.parent.parent
CONFIG_DIR = ROOT_DIR / "scatter" / "pipeline" / "configs"
PIPELINE_DIR = ROOT_DIR / "broadlistening" / "pipeline"


def json_serialize_numpy(obj: Any) -> Any:
    """
    Recursively convert NumPy data types to native Python types for JSON serialization.

    Args:
        obj: Any Python object which might contain NumPy data types

    Returns:
        The same object structure with NumPy types converted to Python native types
    """
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: json_serialize_numpy(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [json_serialize_numpy(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(json_serialize_numpy(item) for item in obj)
    else:
        return obj


class Argument(TypedDict):
    arg_id: str
    argument: str
    comment_id: str
    x: float
    y: float
    p: float
    cluster_ids: list[str]
    attributes: dict[str, str] | None
    url: str | None


class Cluster(TypedDict):
    level: int
    id: str
    label: str
    takeaway: str
    value: int
    parent: str
    density_rank_percentile: float | None


def hierarchical_aggregation(config) -> bool:
    try:
        path = f"outputs/{config['output_dir']}/hierarchical_result.json"
        results = {
            "arguments": [],
            "clusters": [],
            "comments": {},
            "propertyMap": {},
            "translations": {},
            "overview": "",
            "config": config,
        }

        arguments = pd.read_csv(f"outputs/{config['output_dir']}/args.csv")
        arguments.set_index("arg-id", inplace=True)
        arg_num = len(arguments)
        relation_df = pd.read_csv(f"outputs/{config['output_dir']}/relations.csv")
        comments = pd.read_csv(f"inputs/{config['input']}.csv")
        clusters = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")
        labels = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_merge_labels.csv")

        hidden_properties_map: dict[str, list[str]] = config["hierarchical_aggregation"]["hidden_properties"]

        results["arguments"] = _build_arguments(clusters, comments, relation_df, config)
        results["clusters"] = _build_cluster_value(labels, arg_num)

        # results["comments"] = _build_comments_value(
        #     comments, arguments, hidden_properties_map
        # )
        results["comment_num"] = len(comments)
        results["translations"] = _build_translations(config)
        # 属性情報のカラムは、元データに対して指定したカラムとclassificationするカテゴリを合わせたもの
        results["propertyMap"] = _build_property_map(arguments, comments, hidden_properties_map, config)

        with open(f"outputs/{config['output_dir']}/hierarchical_overview.txt") as f:
            overview = f.read()
        print("overview")
        print(overview)
        results["overview"] = overview

        # Convert non-serializable NumPy types to native Python types
        results = json_serialize_numpy(results)

        with open(path, "w") as file:
            json.dump(results, file, indent=2, ensure_ascii=False)
        # TODO: サンプリングロジックを実装したいが、現状は全件抽出
        create_custom_intro(config)
        if config["is_pubcom"]:
            add_original_comments(labels, arguments, relation_df, clusters, config)
        return True
    except Exception as e:
        print("error")
        print(e)
        return False


def create_custom_intro(config):
    dataset = config["output_dir"]
    args_path = PIPELINE_DIR / f"outputs/{dataset}/args.csv"
    comments = pd.read_csv(PIPELINE_DIR / f"inputs/{config['input']}.csv")
    result_path = PIPELINE_DIR / f"outputs/{dataset}/hierarchical_result.json"

    input_count = len(comments)
    args_count = len(pd.read_csv(args_path))
    processed_num = min(input_count, config["extraction"]["limit"])

    print(f"Input count: {input_count}")
    print(f"Args count: {args_count}")

    # LLMプロバイダーとモデル名の判定
    def get_llm_provider_display():
        # configからプロバイダー情報を取得（優先）
        provider = config.get("provider", "openai")
        model = config.get("model", "unknown")

        # プロバイダー名をマッピング
        provider_names = {
            "openai": "OpenAI API",
            "azure": "Azure OpenAI API",
            "openrouter": "OpenRouter API",
            "local": "Local LLM",
        }

        provider_name = provider_names.get(provider, f"{provider} API")
        return f"{provider_name} ({model})"

    llm_provider = get_llm_provider_display()

    base_custom_intro = """{intro}
分析対象となったデータの件数は{processed_num}件で、これらのデータに対して{llm_provider}を用いて{args_count}件の意見（議論）を抽出し、クラスタリングを行った。
"""

    intro = config["intro"]
    custom_intro = base_custom_intro.format(
        intro=intro, processed_num=processed_num, args_count=args_count, llm_provider=llm_provider
    )

    with open(result_path) as f:
        result = json.load(f)
    result["config"]["intro"] = custom_intro
    with open(result_path, "w") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)


def add_original_comments(labels, arguments, relation_df, clusters, config):
    # 大カテゴリ（cluster-level-1）に該当するラベルだけ抽出
    labels_lv1 = labels[labels["level"] == 1][["id", "label"]].rename(
        columns={"id": "cluster-level-1-id", "label": "category_label"}
    )

    # arguments と clusters をマージ（カテゴリ情報付与）
    merged = arguments.merge(clusters[["arg-id", "cluster-level-1-id"]], on="arg-id").merge(
        labels_lv1, on="cluster-level-1-id", how="left"
    )

    # relation_df と結合
    merged = merged.merge(relation_df, on="arg-id", how="left")

    # 元コメント取得
    comments = pd.read_csv(PIPELINE_DIR / f"inputs/{config['input']}.csv")
    comments["comment-id"] = comments["comment-id"].astype(str)
    merged["comment-id"] = merged["comment-id"].astype(str)

    # 元コメント本文などとマージ
    final_df = merged.merge(comments, on="comment-id", how="left")

    # 必要カラムのみ整形
    final_cols = ["comment-id", "comment-body", "arg-id", "argument", "cluster-level-1-id", "category_label"]

    # 基本カラム
    for col in ["x", "y", "source", "url"]:
        if col in comments.columns:
            final_cols.append(col)

    # 属性カラムを追加
    attribute_columns = []
    for col in comments.columns:
        # attributeプレフィックスが付いたカラムを探す
        if col.startswith("attribute_"):
            attribute_columns.append(col)
            final_cols.append(col)

    print(f"属性カラム検出: {attribute_columns}")

    # 必要なカラムだけ選択
    final_df = final_df[final_cols]
    final_df = final_df.rename(
        columns={
            "cluster-level-1-id": "category_id",
            "category_label": "category",
            "arg-id": "arg_id",
            "argument": "argument",
            "comment-body": "original-comment",
        }
    )

    # 保存
    final_df.to_csv(PIPELINE_DIR / f"outputs/{config['output_dir']}/final_result_with_comments.csv", index=False)


def _build_arguments(
    clusters: pd.DataFrame, comments: pd.DataFrame, relation_df: pd.DataFrame, config: dict
) -> list[Argument]:
    """
    Build the arguments list including attribute information from original comments

    Args:
        clusters: DataFrame containing cluster information for each argument
        comments: DataFrame containing original comments with attribute columns
        relation_df: DataFrame relating arguments to original comments
        config: Configuration dictionary containing enable_source_link setting
    """
    cluster_columns = [col for col in clusters.columns if col.startswith("cluster-level-") and "id" in col]

    # Prepare for merging with original comments to get attributes
    comments_copy = comments.copy()
    comments_copy["comment-id"] = comments_copy["comment-id"].astype(str)

    # Get argument to comment mapping
    arg_comment_map = {}
    if "comment-id" in relation_df.columns:
        relation_df["comment-id"] = relation_df["comment-id"].astype(str)
        arg_comment_map = dict(zip(relation_df["arg-id"], relation_df["comment-id"], strict=False))

    # Find attribute columns in comments dataframe
    attribute_columns = [col for col in comments.columns if col.startswith("attribute_")]
    print(f"属性カラム検出: {attribute_columns}")

    arguments: list[Argument] = []
    for _, row in clusters.iterrows():
        cluster_ids = ["0"]
        for cluster_column in cluster_columns:
            cluster_ids.append(str(row[cluster_column]))  # Convert to string to ensure serializable

        # Create base argument
        argument: Argument = {
            "arg_id": str(row["arg-id"]),  # Convert to string to ensure serializable
            "argument": str(row["argument"]),
            "x": float(row["x"]),  # Convert to native float
            "y": float(row["y"]),  # Convert to native float
            "p": 0,  # NOTE: 一旦全部0でいれる
            "cluster_ids": cluster_ids,
            "attributes": None,
            "url": None,
        }

        # Add attributes and URL if available
        if row["arg-id"] in arg_comment_map:
            comment_id = arg_comment_map[row["arg-id"]]
            comment_rows = comments_copy[comments_copy["comment-id"] == comment_id]

            if not comment_rows.empty:
                comment_row = comment_rows.iloc[0]

                # Add URL if available and enabled
                if config.get("enable_source_link", False) and "url" in comment_row and comment_row["url"] is not None:
                    argument["url"] = str(comment_row["url"])

                # Add attributes if available
                if attribute_columns:
                    attributes = {}
                    for attr_col in attribute_columns:
                        # Remove "attribute_" prefix for cleaner attribute names
                        attr_name = attr_col[len("attribute_") :]
                        # Convert potential numpy types to Python native types
                        attr_value = comment_row.get(attr_col, None)
                        if attr_value is not None:
                            if isinstance(attr_value, np.integer):
                                attr_value = int(attr_value)
                            elif isinstance(attr_value, np.floating):
                                attr_value = float(attr_value)
                            elif isinstance(attr_value, np.ndarray):
                                attr_value = attr_value.tolist()
                        attributes[attr_name] = attr_value

                    # Only add non-empty attributes
                    if any(v is not None for v in attributes.values()):
                        argument["attributes"] = attributes

        arguments.append(argument)
    return arguments


def _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:
    results: list[Cluster] = [
        Cluster(
            level=0,
            id="0",
            label="全体",
            takeaway="",
            value=int(total_num),  # Convert to native int
            parent="",
            density_rank_percentile=0,
        )
    ]

    for _, melted_label in melted_labels.iterrows():
        # Convert potential NumPy types to native Python types
        level = (
            int(melted_label["level"]) if isinstance(melted_label["level"], int | np.integer) else melted_label["level"]
        )
        cluster_id = str(melted_label["id"])
        label = str(melted_label["label"])
        takeaway = str(melted_label["description"])
        value = (
            int(melted_label["value"]) if isinstance(melted_label["value"], int | np.integer) else melted_label["value"]
        )
        parent = str(melted_label.get("parent", "全体"))

        # Handle density_rank_percentile which might be None or a numeric value
        density_rank = melted_label.get("density_rank_percentile")
        if density_rank is not None:
            if isinstance(density_rank, float | np.floating):
                density_rank = float(density_rank)
            elif isinstance(density_rank, int | np.integer):
                density_rank = int(density_rank)

        cluster_value = Cluster(
            level=level,
            id=cluster_id,
            label=label,
            takeaway=takeaway,
            value=value,
            parent=parent,
            density_rank_percentile=density_rank,
        )
        results.append(cluster_value)
    return results


def _build_comments_value(
    comments: pd.DataFrame,
    arguments: pd.DataFrame,
    hidden_properties_map: dict[str, list[str]],
):
    comment_dict: dict[str, dict[str, str]] = {}
    useful_comment_ids = set(arguments["comment-id"].values)
    for _, row in comments.iterrows():
        id = row["comment-id"]
        if id in useful_comment_ids:
            res = {"comment": row["comment-body"]}
            should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())
            if should_skip:
                continue
            comment_dict[str(id)] = res

    return comment_dict


def _build_translations(config):
    languages = list(config.get("translation", {}).get("languages", []))
    if len(languages) > 0:
        with open(PIPELINE_DIR / f"outputs/{config['output_dir']}/translations.json") as f:
            translations = f.read()
        return json.loads(translations)
    return {}


def _build_property_map(
    arguments: pd.DataFrame, comments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict
) -> dict[str, dict[str, str]]:
    property_columns = list(hidden_properties_map.keys()) + list(config["extraction"]["categories"].keys())
    property_map = defaultdict(dict)

    # 指定された property_columns が arguments に存在するかチェック
    missing_cols = [col for col in property_columns if col not in arguments.columns]
    if missing_cols:
        raise ValueError(
            f"指定されたカラム {missing_cols} が args.csv に存在しません。"
            "設定ファイルaggregation / hidden_propertiesから該当カラムを取り除いてください。"
        )

    for prop in property_columns:
        for arg_id, row in arguments.iterrows():
            # LLMによるcategory classificationがうまく行かず、NaNの場合はNoneにする
            value = row[prop] if not pd.isna(row[prop]) else None

            # Convert NumPy types to Python native types
            if value is not None:
                if isinstance(value, np.integer):
                    value = int(value)
                elif isinstance(value, np.floating):
                    value = float(value)
                elif isinstance(value, np.ndarray):
                    value = value.tolist()
                else:
                    # Convert any other types to string to ensure serialization
                    try:
                        value = str(value)
                    except Exception as e:
                        print(f"Error converting value to string: {e}")
                        value = None

            # Make sure arg_id is string
            str_arg_id = str(arg_id)
            property_map[prop][str_arg_id] = value

    return property_map
1f:T458,import os

import pandas as pd
from tqdm import tqdm

from services.llm import request_to_embed


def embedding(config):
    model = config["embedding"]["model"]
    is_embedded_at_local = config["is_embedded_at_local"]
    # print("start embedding")
    # print(f"embedding model: {model}, is_embedded_at_local: {is_embedded_at_local}")

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/embeddings.pkl"
    arguments = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings = []
    batch_size = 1000
    for i in tqdm(range(0, len(arguments), batch_size)):
        args = arguments["argument"].tolist()[i : i + batch_size]
        embeds = request_to_embed(
            args,
            model,
            is_embedded_at_local,
            config["provider"],
            local_llm_address=config.get("local_llm_address"),
            user_api_key=os.getenv("USER_API_KEY"),
        )
        embeddings.extend(embeds)
    df = pd.DataFrame([{"arg-id": arguments.iloc[i]["arg-id"], "embedding": e} for i, e in enumerate(embeddings)])
    df.to_pickle(path)
20:T194c,import concurrent.futures
import json
import logging
import os
import re

import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai
from services.parse_json_list import parse_extraction_response
from utils import update_progress

COMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r",\s*(\])")


class ExtractionResponse(BaseModel):
    extractedOpinionList: list[str] = Field(..., description="抽出した意見のリスト")


def _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:
    if not all(property in comments.columns for property in property_columns):
        raise ValueError(f"Properties {property_columns} not found in comments. Columns are {comments.columns}")


def extraction(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/args.csv"
    model = config["extraction"]["model"]
    prompt = config["extraction"]["prompt"]
    workers = config["extraction"]["workers"]
    limit = config["extraction"]["limit"]
    property_columns = config["extraction"]["properties"]

    if "provider" not in config:
        raise RuntimeError("provider is not set")
    provider = config["provider"]

    # カラム名だけを読み込み、必要なカラムが含まれているか確認する
    comments = pd.read_csv(f"inputs/{config['input']}.csv", nrows=0)
    _validate_property_columns(property_columns, comments)
    # エラーが出なかった場合、すべての行を読み込む
    comments = pd.read_csv(
        f"inputs/{config['input']}.csv", usecols=["comment-id", "comment-body"] + config["extraction"]["properties"]
    )
    comment_ids = (comments["comment-id"].values)[:limit]
    comments.set_index("comment-id", inplace=True)
    results = pd.DataFrame()
    update_progress(config, total=len(comment_ids))

    argument_map = {}
    relation_rows = []

    for i in tqdm(range(0, len(comment_ids), workers)):
        batch = comment_ids[i : i + workers]
        batch_inputs = [comments.loc[id]["comment-body"] for id in batch]
        batch_results = extract_batch(
            batch_inputs, prompt, model, workers, provider, config.get("local_llm_address"), config
        )

        for comment_id, extracted_args in zip(batch, batch_results, strict=False):
            for j, arg in enumerate(extracted_args):
                if arg not in argument_map:
                    # argumentテーブルに追加
                    arg_id = f"A{comment_id}_{j}"
                    argument = arg
                    argument_map[arg] = {
                        "arg-id": arg_id,
                        "argument": argument,
                    }
                else:
                    arg_id = argument_map[arg]["arg-id"]

                # relationテーブルにcommentとargの関係を追加
                relation_row = {
                    "arg-id": arg_id,
                    "comment-id": comment_id,
                }
                relation_rows.append(relation_row)

        update_progress(config, incr=len(batch))

    # DataFrame化
    results = pd.DataFrame(argument_map.values())
    relation_df = pd.DataFrame(relation_rows)

    if results.empty:
        raise RuntimeError("result is empty, maybe bad prompt")

    results.to_csv(path, index=False)
    # comment-idとarg-idの関係を保存
    relation_df.to_csv(f"outputs/{dataset}/relations.csv", index=False)


logging.basicConfig(level=logging.DEBUG)


def extract_batch(batch, prompt, model, workers, provider="openai", local_llm_address=None, config=None):
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures_with_index = [
            (i, executor.submit(extract_arguments, input, prompt, model, provider, local_llm_address))
            for i, input in enumerate(batch)
        ]

        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)
        results = [[] for _ in range(len(batch))]
        total_token_input = 0
        total_token_output = 0
        total_token_usage = 0

        for _, future in futures_with_index:
            if future in not_done and not future.cancelled():
                future.cancel()

        for i, future in futures_with_index:
            if future in done:
                try:
                    result = future.result()
                    if isinstance(result, tuple) and len(result) == 4:
                        items, token_input, token_output, token_total = result
                        results[i] = items
                        total_token_input += token_input
                        total_token_output += token_output
                        total_token_usage += token_total
                    else:
                        results[i] = result
                except Exception as e:
                    logging.error(f"Task {future} failed with error: {e}")
                    results[i] = []

        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + total_token_usage
            config["token_usage_input"] = config.get("token_usage_input", 0) + total_token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + total_token_output
            print(
                f"Extraction batch: input={total_token_input}, output={total_token_output}, total={total_token_usage} tokens"
            )

        return results


def extract_arguments(input, prompt, model, provider="openai", local_llm_address=None):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            is_json=False,
            json_schema=ExtractionResponse,
            provider=provider,
            local_llm_address=local_llm_address,
            user_api_key=os.getenv("USER_API_KEY"),
        )
        items = parse_extraction_response(response)
        items = list(filter(None, items))  # omit empty strings
        return items, token_input, token_output, token_total
    except json.decoder.JSONDecodeError as e:
        print("JSON error:", e)
        print("Input was:", input)
        print("Response was:", response)
        print("Silently giving up on trying to generate valid list.")
        return []
21:T458,import os

import pandas as pd
from tqdm import tqdm

from services.llm import request_to_embed


def embedding(config):
    model = config["embedding"]["model"]
    is_embedded_at_local = config["is_embedded_at_local"]
    # print("start embedding")
    # print(f"embedding model: {model}, is_embedded_at_local: {is_embedded_at_local}")

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/embeddings.pkl"
    arguments = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings = []
    batch_size = 1000
    for i in tqdm(range(0, len(arguments), batch_size)):
        args = arguments["argument"].tolist()[i : i + batch_size]
        embeds = request_to_embed(
            args,
            model,
            is_embedded_at_local,
            config["provider"],
            local_llm_address=config.get("local_llm_address"),
            user_api_key=os.getenv("USER_API_KEY"),
        )
        embeddings.extend(embeds)
    df = pd.DataFrame([{"arg-id": arguments.iloc[i]["arg-id"], "embedding": e} for i, e in enumerate(embeddings)])
    df.to_pickle(path)
22:T1149,"""Cluster the arguments using UMAP + HDBSCAN and GPT-4."""

from importlib import import_module

import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans


def hierarchical_clustering(config):
    UMAP = import_module("umap").UMAP

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_clusters.csv"
    arguments_df = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings_df = pd.read_pickle(f"outputs/{dataset}/embeddings.pkl")
    embeddings_array = np.asarray(embeddings_df["embedding"].values.tolist())
    cluster_nums = config["hierarchical_clustering"]["cluster_nums"]

    n_samples = embeddings_array.shape[0]
    # デフォルト設定は15
    default_n_neighbors = 15

    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる
    if n_samples <= default_n_neighbors:
        n_neighbors = max(2, n_samples - 1)  # 最低2以上
    else:
        n_neighbors = default_n_neighbors

    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)
    # TODO 詳細エラーメッセージを加える
    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因
    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
    umap_embeds = umap_model.fit_transform(embeddings_array)

    cluster_results = hierarchical_clustering_embeddings(
        umap_embeds=umap_embeds,
        cluster_nums=cluster_nums,
    )
    result_df = pd.DataFrame(
        {
            "arg-id": arguments_df["arg-id"],
            "argument": arguments_df["argument"],
            "x": umap_embeds[:, 0],
            "y": umap_embeds[:, 1],
        }
    )

    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):
        result_df[f"cluster-level-{cluster_level}-id"] = [f"{cluster_level}_{label}" for label in final_labels]

    result_df.to_csv(path, index=False)


def generate_cluster_count_list(min_clusters: int, max_clusters: int):
    cluster_counts = []
    current = min_clusters
    cluster_counts.append(current)

    if min_clusters == max_clusters:
        return cluster_counts

    while True:
        next_double = current * 2
        next_triple = current * 3

        if next_double >= max_clusters:
            if cluster_counts[-1] != max_clusters:
                cluster_counts.append(max_clusters)
            break

        # 次の倍はまだ max_clusters に収まるが、3倍だと超える
        # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ
        if next_triple > max_clusters:
            cluster_counts.append(max_clusters)
            break

        cluster_counts.append(next_double)
        current = next_double

    return cluster_counts


def merge_clusters_with_hierarchy(
    cluster_centers: np.ndarray,
    kmeans_labels: np.ndarray,
    umap_array: np.ndarray,
    n_cluster_cut: int,
):
    Z = sch.linkage(cluster_centers, method="ward")
    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion="maxclust")

    n_samples = umap_array.shape[0]
    final_labels = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        original_label = kmeans_labels[i]
        final_labels[i] = cluster_labels_merged[original_label]

    return final_labels


def hierarchical_clustering_embeddings(
    umap_embeds,
    cluster_nums,
):
    # 最大分割数でクラスタリングを実施
    print("start initial clustering")
    initial_cluster_num = cluster_nums[-1]
    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)
    kmeans_model.fit(umap_embeds)
    print("end initial clustering")

    results = {}
    print("start hierarchical clustering")
    cluster_nums.sort()
    print(cluster_nums)
    for n_cluster_cut in cluster_nums[:-1]:
        print("n_cluster_cut: ", n_cluster_cut)
        final_labels = merge_clusters_with_hierarchy(
            cluster_centers=kmeans_model.cluster_centers_,
            kmeans_labels=kmeans_model.labels_,
            umap_array=umap_embeds,
            n_cluster_cut=n_cluster_cut,
        )
        results[n_cluster_cut] = final_labels

    results[initial_cluster_num] = kmeans_model.labels_
    print("end hierarchical clustering")

    return results
23:T1c36,import json
import os
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import TypedDict

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class LabellingResult(TypedDict):
    """各クラスタのラベリング結果を表す型"""

    cluster_id: str  # クラスタのID
    label: str  # クラスタのラベル名
    description: str  # クラスタの説明文


def hierarchical_initial_labelling(config: dict) -> None:
    """階層的クラスタリングの初期ラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_initial_labelling: 初期ラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_initial_labels.csv"
    clusters_argument_df = pd.read_csv(f"outputs/{dataset}/hierarchical_clusters.csv")

    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith("cluster-level-")]
    initial_cluster_id_column = cluster_id_columns[-1]
    sampling_num = config["hierarchical_initial_labelling"]["sampling_num"]
    initial_labelling_prompt = config["hierarchical_initial_labelling"]["prompt"]
    model = config["hierarchical_initial_labelling"]["model"]
    workers = config["hierarchical_initial_labelling"]["workers"]

    # トークン使用量を追跡するための変数を初期化
    config["total_token_usage"] = config.get("total_token_usage", 0)

    initial_label_df = initial_labelling(
        initial_labelling_prompt,
        clusters_argument_df,
        sampling_num,
        model,
        workers,
        config["provider"],
        config.get("local_llm_address"),
        config,  # configを渡して、トークン使用量を累積できるようにする
    )
    print("start initial labelling")
    initial_clusters_argument_df = clusters_argument_df.merge(
        initial_label_df,
        left_on=initial_cluster_id_column,
        right_on="cluster_id",
        how="left",
    ).rename(
        columns={
            "label": f"{initial_cluster_id_column.replace('-id', '')}-label",
            "description": f"{initial_cluster_id_column.replace('-id', '')}-description",
        }
    )
    print("end initial labelling")
    initial_clusters_argument_df.to_csv(path, index=False)


def initial_labelling(
    prompt: str,
    clusters_df: pd.DataFrame,
    sampling_num: int,
    model: str,
    workers: int,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> pd.DataFrame:
    """各クラスタに対して初期ラベリングを実行する

    Args:
        prompt: LLMへのプロンプト
        clusters_df: クラスタリング結果のDataFrame
        sampling_num: 各クラスタからサンプリングする意見の数
        model: 使用するLLMモデル名
        workers: 並列処理のワーカー数
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        各クラスタのラベリング結果を含むDataFrame
    """
    cluster_columns = [col for col in clusters_df.columns if col.startswith("cluster-level-")]
    initial_cluster_column = cluster_columns[-1]
    cluster_ids = clusters_df[initial_cluster_column].unique()
    process_func = partial(
        process_initial_labelling,
        df=clusters_df,
        prompt=prompt,
        sampling_num=sampling_num,
        target_column=initial_cluster_column,
        model=model,
        provider=provider,
        local_llm_address=local_llm_address,
        config=config,  # configを渡す
    )
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_func, cluster_ids))
    return pd.DataFrame(results)


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_initial_labelling(
    cluster_id: str,
    df: pd.DataFrame,
    prompt: str,
    sampling_num: int,
    target_column: str,
    model: str,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> LabellingResult:
    """個別のクラスタに対してラベリングを実行する

    Args:
        cluster_id: 処理対象のクラスタID
        df: クラスタリング結果のDataFrame
        prompt: LLMへのプロンプト
        sampling_num: サンプリングする意見の数
        target_column: クラスタIDが格納されている列名
        model: 使用するLLMモデル名
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        クラスタのラベリング結果
    """
    cluster_data = df[df[target_column] == cluster_id]
    sampling_num = min(sampling_num, len(cluster_data))
    cluster = cluster_data.sample(sampling_num)
    input = "\n".join(cluster["argument"].values)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            provider=provider,
            json_schema=LabellingFromat,
            local_llm_address=local_llm_address,
            user_api_key=os.getenv("USER_API_KEY"),
        )

        # トークン使用量を累積（configが渡されている場合）
        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
            config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + token_output

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return LabellingResult(
            cluster_id=cluster_id,
            label=response_json.get("label", "エラーでラベル名が取得できませんでした"),
            description=response_json.get("description", "エラーで解説が取得できませんでした"),
        )
    except Exception as e:
        print(e)
        return LabellingResult(
            cluster_id=cluster_id,
            label="エラーでラベル名が取得できませんでした",
            description="エラーで解説が取得できませんでした",
        )
24:T337b,import json
import os
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from functools import partial

import numpy as np
import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai


@dataclass
class ClusterColumns:
    """同一階層のクラスター関連のカラム名を管理するクラス"""

    id: str
    label: str
    description: str

    @classmethod
    def from_id_column(cls, id_column: str) -> "ClusterColumns":
        """ID列名から関連するカラム名を生成"""
        return cls(
            id=id_column,
            label=id_column.replace("-id", "-label"),
            description=id_column.replace("-id", "-description"),
        )


@dataclass
class ClusterValues:
    """対象クラスタのlabel/descriptionを管理するクラス"""

    label: str
    description: str

    def to_prompt_text(self) -> str:
        return f"- {self.label}: {self.description}"


def hierarchical_merge_labelling(config: dict) -> None:
    """階層的クラスタリングの結果に対してマージラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_merge_labelling: マージラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    merge_path = f"outputs/{dataset}/hierarchical_merge_labels.csv"
    clusters_df = pd.read_csv(f"outputs/{dataset}/hierarchical_initial_labels.csv")

    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)
    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成
    merge_result_df = merge_labelling(
        clusters_df=clusters_df,
        cluster_id_columns=sorted(cluster_id_columns, reverse=True),
        config=config,
    )
    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成
    melted_df = melt_cluster_data(merge_result_df)
    # 上記のdfに親子関係を追加
    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)
    melted_df = melted_df.merge(parent_child_df, on=["level", "id"], how="left")
    density_df = calculate_cluster_density(melted_df, config)
    density_df.to_csv(merge_path, index=False)


def _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):
    """クラスタ間の親子関係をマッピングする

    Args:
        df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト

    Returns:
        親子関係のマッピング情報を含むDataFrame
    """
    results = []
    top_cluster_column = cluster_id_columns[0]
    top_cluster_values = df[top_cluster_column].unique()
    for c in top_cluster_values:
        results.append(
            {
                "level": 1,
                "id": c,
                "parent": "0",  # aggregationで追加する全体クラスタのid
            }
        )

    for idx in range(len(cluster_id_columns) - 1):
        current_column = cluster_id_columns[idx]
        children_column = cluster_id_columns[idx + 1]
        current_level = current_column.replace("-id", "").replace("cluster-level-", "")
        # 現在のレベルのクラスタid
        current_cluster_values = df[current_column].unique()
        for current_id in current_cluster_values:
            children_ids = df.loc[df[current_column] == current_id, children_column].unique()
            for child_id in children_ids:
                results.append(
                    {
                        "level": int(current_level) + 1,
                        "id": child_id,
                        "parent": current_id,
                    }
                )
    return pd.DataFrame(results)


def _filter_id_columns(columns: list[str]) -> list[str]:
    """クラスタIDのカラム名をフィルタリングする

    Args:
        columns: 全カラム名のリスト

    Returns:
        クラスタIDのカラム名のリスト
    """
    return [col for col in columns if col.startswith("cluster-level-") and col.endswith("-id")]


def melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:
    """クラスタデータを行形式に変換する

    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。
    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。

    Args:
        df: クラスタリング結果のDataFrame

    Returns:
        行形式に変換されたDataFrame
    """
    id_columns: list[str] = _filter_id_columns(df.columns)
    levels: set[int] = {int(col.replace("cluster-level-", "").replace("-id", "")) for col in id_columns}
    all_rows: list[dict] = []

    # levelごとに各クラスタの出現件数を集計・縦持ちにする
    for level in levels:
        cluster_columns = ClusterColumns.from_id_column(f"cluster-level-{level}-id")
        # クラスタidごとの件数集計
        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name="value")

        level_unique_val_df = df[
            [cluster_columns.id, cluster_columns.label, cluster_columns.description]
        ].drop_duplicates()
        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how="left")
        level_unique_vals = [
            {
                "level": level,
                "id": row[cluster_columns.id],
                "label": row[cluster_columns.label],
                "description": row[cluster_columns.description],
                "value": row["value"],
            }
            for _, row in level_unique_val_df.iterrows()
        ]
        all_rows.extend(level_unique_vals)
    return pd.DataFrame(all_rows)


def merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:
    """階層的なクラスタのマージラベリングを実行する

    Args:
        clusters_df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含むDataFrame
    """
    for idx in tqdm(range(len(cluster_id_columns) - 1)):
        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])
        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])

        process_fn = partial(
            process_merge_labelling,
            result_df=clusters_df,
            current_columns=current_columns,
            previous_columns=previous_columns,
            config=config,
        )

        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())
        with ThreadPoolExecutor(max_workers=config["hierarchical_merge_labelling"]["workers"]) as executor:
            responses = list(
                tqdm(
                    executor.map(process_fn, current_cluster_ids),
                    total=len(current_cluster_ids),
                )
            )

        current_result_df = pd.DataFrame(responses)
        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])
    return clusters_df


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_merge_labelling(
    target_cluster_id: str,
    result_df: pd.DataFrame,
    current_columns: ClusterColumns,
    previous_columns: ClusterColumns,
    config,
):
    """個別のクラスタに対してマージラベリングを実行する

    Args:
        target_cluster_id: 処理対象のクラスタID
        result_df: クラスタリング結果のDataFrame
        current_columns: 現在のレベルのカラム情報
        previous_columns: 前のレベルのカラム情報
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含む辞書
    """

    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:
        """前のレベルのクラスタ情報を取得する"""
        previous_records = df[df[current_columns.id] == target_cluster_id][
            [previous_columns.label, previous_columns.description]
        ].drop_duplicates()
        previous_values = [
            ClusterValues(
                label=row[previous_columns.label],
                description=row[previous_columns.description],
            )
            for _, row in previous_records.iterrows()
        ]
        return previous_values

    previous_values = filter_previous_values(result_df, previous_columns)
    if len(previous_values) == 1:
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: previous_values[0].label,
            current_columns.description: previous_values[0].description,
        }
    elif len(previous_values) == 0:
        raise ValueError(f"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。")

    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]
    sampling_num = min(
        config["hierarchical_merge_labelling"]["sampling_num"],
        len(current_cluster_data),
    )
    sampled_data = current_cluster_data.sample(sampling_num)
    sampled_argument_text = "\n".join(sampled_data["argument"].values)
    cluster_text = "\n".join([value.to_prompt_text() for value in previous_values])
    messages = [
        {"role": "system", "content": config["hierarchical_merge_labelling"]["prompt"]},
        {
            "role": "user",
            "content": "クラスタラベル\n" + cluster_text + "\n" + "クラスタの意見\n" + sampled_argument_text,
        },
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=config["hierarchical_merge_labelling"]["model"],
            json_schema=LabellingFromat,
            provider=config["provider"],
            local_llm_address=config.get("local_llm_address"),
            user_api_key=os.getenv("USER_API_KEY"),
        )

        config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
        config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
        config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
        print(f"Merge labelling: input={token_input}, output={token_output}, total={token_total} tokens")

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: response_json.get("label", "エラーでラベル名が取得できませんでした"),
            current_columns.description: response_json.get("description", "エラーで解説が取得できませんでした"),
        }
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: "エラーでラベル名が取得できませんでした",
            current_columns.description: "エラーで解説が取得できませんでした",
        }


def calculate_cluster_density(melted_df: pd.DataFrame, config: dict):
    """クラスタ内の密度計算"""
    hierarchical_cluster_df = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")

    densities = []
    for level, c_id in zip(melted_df["level"], melted_df["id"], strict=False):
        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f"cluster-level-{level}-id"] == c_id][
            ["x", "y"]
        ].values
        density = calculate_density(cluster_embeds)
        densities.append(density)

    # 密度のランクを計算
    melted_df["density"] = densities
    melted_df["density_rank"] = melted_df.groupby("level")["density"].rank(ascending=False, method="first")
    melted_df["density_rank_percentile"] = melted_df.groupby("level")["density_rank"].transform(lambda x: x / len(x))
    return melted_df


def calculate_density(embeds: np.ndarray):
    """平均距離に基づいて密度を計算"""
    center = np.mean(embeds, axis=0)
    distances = np.linalg.norm(embeds - center, axis=1)
    avg_distance = np.mean(distances)
    density = 1 / (avg_distance + 1e-10)
    return density
25:Ta82,"""Create summaries for the clusters."""

import json
import os
import re

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class OverviewResponse(BaseModel):
    summary: str = Field(..., description="クラスターの全体的な要約")


def hierarchical_overview(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_overview.txt"

    hierarchical_label_df = pd.read_csv(f"outputs/{dataset}/hierarchical_merge_labels.csv")

    prompt = config["hierarchical_overview"]["prompt"]
    model = config["hierarchical_overview"]["model"]

    # TODO: level1で固定にしているが、設定で変えられるようにする
    target_level = 1
    target_records = hierarchical_label_df[hierarchical_label_df["level"] == target_level]
    ids = target_records["id"].to_list()
    labels = target_records["label"].to_list()
    descriptions = target_records["description"].to_list()
    target_records.set_index("id", inplace=True)

    input_text = ""
    for i, _ in enumerate(ids):
        input_text += f"# Cluster {i}/{len(ids)}: {labels[i]}\n\n"
        input_text += descriptions[i] + "\n\n"

    messages = [{"role": "system", "content": prompt}, {"role": "user", "content": input_text}]
    response_text, token_input, token_output, token_total = request_to_chat_ai(
        messages=messages,
        model=model,
        provider=config["provider"],
        local_llm_address=config.get("local_llm_address"),
        user_api_key=os.getenv("USER_API_KEY"),
        json_schema=OverviewResponse,
    )

    # トークン使用量を累積
    config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
    config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
    config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
    print(f"Hierarchical overview: input={token_input}, output={token_output}, total={token_total} tokens")

    try:
        # structured outputとしてパースできるなら処理する
        if isinstance(response_text, dict):
            parsed_response = response_text
        else:
            parsed_response = json.loads(response_text)

        with open(path, "w") as file:
            file.write(parsed_response["summary"])

    except Exception:
        # thinkタグが出力されるReasoningモデル用に、thinkタグを除去する
        thinking_removed = re.sub(
            r"<think\b[^>]*>.*?</think>",
            "",
            response_text,
            flags=re.DOTALL,
        )

        with open(path, "w") as file:
            file.write(thinking_removed)
8:[["$","$L13",null,{}],["$","$L14",null,{"className":"container","mt":"8","children":[["$","$L14",null,{"mx":"auto","maxW":"750px","mb":8,"children":[["$","$L15",null,{"textAlign":"left","fontSize":"xl","mb":5,"children":"レポート"}],["$","$L15",null,{"as":"h2","size":"4xl","mb":2,"className":"headingColor","children":"③市民として、2040年のありたい姿とは【2/15 15:00更新】"}],["$","$L16",null,{"fontWeight":"bold","fontSize":"xl","mb":2,"children":[["$","$L17",null,{"mr":1,"children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":20,"height":20,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-messages-square","children":[["$","path","p1xzt8",{"d":"M14 9a2 2 0 0 1-2 2H6l-4 4V4a2 2 0 0 1 2-2h8a2 2 0 0 1 2 2z"}],["$","path","1cx29u",{"d":"M18 9h2a2 2 0 0 1 2 2v11l-4-4h-6a2 2 0 0 1-2-2v-1"}],"$undefined"]}]}],"99","件"]}],["$","p",null,{"children":"舞鶴市では、住環境の改善や地域活性化に対する期待が高まり、市民は快適な生活環境や就職機会の増加を求めています。また、健康的で自由なライフスタイルや地域コミュニティの重要性が強調され、持続可能な社会の実現が求められています。共生と尊重に基づく平和な社会を目指し、世代を超えた活力あるまちづくりが進められています。市民の幸福を実現するため、住みやすい街づくりや地域の魅力向上が重要視されています。"}]]}],["$","$L18",null,{"result":{"arguments":[{"arg_id":"Acsv-1_0","argument":"病気になっても舞鶴で暮らせてよかったと思いたい。","x":-3.0802557,"y":11.803573,"p":0,"cluster_ids":["0","1_7","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-6_0","argument":"健康第一。","x":-3.9012275,"y":9.776793,"p":0,"cluster_ids":["0","1_4","2_41"],"attributes":null,"url":null},{"arg_id":"Acsv-9_0","argument":"自己肯定感の高い人が多い","x":-5.2889943,"y":10.067944,"p":0,"cluster_ids":["0","1_4","2_62"],"attributes":null,"url":null},{"arg_id":"Acsv-14_0","argument":"地域活動の場に多様な人材が関わり合うべきである。","x":-5.2043495,"y":8.296222,"p":0,"cluster_ids":["0","1_1","2_31"],"attributes":null,"url":null},{"arg_id":"Acsv-19_0","argument":"想像できない","x":-4.41775,"y":7.5983844,"p":0,"cluster_ids":["0","1_3","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-20_0","argument":"争いの少ない世の中を目指すべき","x":-6.860716,"y":8.7548275,"p":0,"cluster_ids":["0","1_2","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-20_1","argument":"犯罪率の少ない社会を実現する必要がある","x":-6.7195315,"y":8.220562,"p":0,"cluster_ids":["0","1_2","2_33"],"attributes":null,"url":null},{"arg_id":"Acsv-27_0","argument":"娯楽が溢れる世の中である","x":-5.2120023,"y":7.4731774,"p":0,"cluster_ids":["0","1_3","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-28_0","argument":"一度失敗しても、許してくれる人になりたい。","x":-3.7108364,"y":7.467054,"p":0,"cluster_ids":["0","1_3","2_25"],"attributes":null,"url":null},{"arg_id":"Acsv-29_0","argument":"新しくできたスタバで働きたい","x":-3.773489,"y":11.876172,"p":0,"cluster_ids":["0","1_7","2_30"],"attributes":null,"url":null},{"arg_id":"Acsv-30_0","argument":"多様な人がお互いを尊重し気持ちよく生きられる社会が必要である。","x":-6.0518823,"y":7.8267045,"p":0,"cluster_ids":["0","1_2","2_59"],"attributes":null,"url":null},{"arg_id":"Acsv-322_0","argument":"不安のない日々を求めるべきである。","x":-3.4491744,"y":9.274284,"p":0,"cluster_ids":["0","1_5","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-323_0","argument":"多文化共生を終わらせるべきである。","x":-6.2842493,"y":7.6786313,"p":0,"cluster_ids":["0","1_2","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-323_1","argument":"日本人にばかり我慢を強いることは許されない。","x":-4.1681843,"y":7.905072,"p":0,"cluster_ids":["0","1_3","2_48"],"attributes":null,"url":null},{"arg_id":"Acsv-323_2","argument":"子どもたちに迷惑をかけないようにすべきである。","x":-3.7458017,"y":9.347018,"p":0,"cluster_ids":["0","1_5","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-324_0","argument":"それぞれが自分の特性に応じた生き方ができるべきである。","x":-5.070027,"y":9.265329,"p":0,"cluster_ids":["0","1_4","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-324_1","argument":"人を馬鹿にせず、馬鹿にすることで相手に依存しない生き方が重要である。","x":-4.660026,"y":8.21892,"p":0,"cluster_ids":["0","1_1","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-324_2","argument":"それぞれが自立した生き方をすることが大切である。","x":-5.4906645,"y":8.950375,"p":0,"cluster_ids":["0","1_1","2_47"],"attributes":null,"url":null},{"arg_id":"Acsv-336_0","argument":"高齢者が車の運転ができなくても自由にまちを行き来できる環境が必要である","x":-4.4255295,"y":10.661078,"p":0,"cluster_ids":["0","1_8","2_29"],"attributes":null,"url":null},{"arg_id":"Acsv-336_1","argument":"安心して医療を受けられるまちであるべきである","x":-3.5251691,"y":9.705957,"p":0,"cluster_ids":["0","1_5","2_43"],"attributes":null,"url":null},{"arg_id":"Acsv-338_0","argument":"舞鶴海洋少年団での活動を通じて、舞鶴で育って良かったと思ってもらえるようにしたい。","x":-2.8458881,"y":11.9288645,"p":0,"cluster_ids":["0","1_7","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-358_0","argument":"市民が住みやすいと思えるまちを作るべき","x":-5.533969,"y":11.415658,"p":0,"cluster_ids":["0","1_6","2_36"],"attributes":null,"url":null},{"arg_id":"Acsv-358_1","argument":"市民が出て行こうと思わないまちを目指すべき","x":-5.7230544,"y":11.188029,"p":0,"cluster_ids":["0","1_6","2_36"],"attributes":null,"url":null},{"arg_id":"Acsv-358_2","argument":"持続可能なまちを実現する必要がある","x":-5.2993217,"y":11.672312,"p":0,"cluster_ids":["0","1_8","2_63"],"attributes":null,"url":null},{"arg_id":"Acsv-359_0","argument":"若者が将来に夢を持てる環境を整えるべき","x":-4.196457,"y":11.1727495,"p":0,"cluster_ids":["0","1_8","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-359_1","argument":"舞鶴市内に就職先を増やすべき","x":-3.1885724,"y":12.054048,"p":0,"cluster_ids":["0","1_7","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-359_2","argument":"年配の方が安心して暮らせる社会を実現すべき","x":-3.584485,"y":10.274371,"p":0,"cluster_ids":["0","1_8","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-361_0","argument":"多様な人がその人らしく暮らせることは守られるべき尊厳である。","x":-5.7383766,"y":8.053084,"p":0,"cluster_ids":["0","1_2","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-361_1","argument":"障害や病気、高齢に関わらず「自分で選べる、自分でできる」ことが守られるべきである。","x":-5.0272465,"y":9.755011,"p":0,"cluster_ids":["0","1_4","2_42"],"attributes":null,"url":null},{"arg_id":"Acsv-361_2","argument":"舞鶴が「人を大事にできる」人が大勢になることを願う。","x":-2.906959,"y":11.883319,"p":0,"cluster_ids":["0","1_7","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-361_3","argument":"人を大事にできる町になれば争いも生まれず、大きな派閥に苦しむこともない。","x":-5.907232,"y":10.403202,"p":0,"cluster_ids":["0","1_6","2_58"],"attributes":null,"url":null},{"arg_id":"Acsv-361_4","argument":"みんなが自分の得意を活かす町になれると思う。","x":-5.440321,"y":10.533342,"p":0,"cluster_ids":["0","1_6","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-363_0","argument":"光熱水を自給自足することが重要である。","x":-5.5951085,"y":9.194901,"p":0,"cluster_ids":["0","1_1","2_47"],"attributes":null,"url":null},{"arg_id":"Acsv-363_1","argument":"投資で得たお金を使って通販や外食、映画鑑賞を楽しむべきである。","x":-4.501231,"y":10.183148,"p":0,"cluster_ids":["0","1_4","2_38"],"attributes":null,"url":null},{"arg_id":"Acsv-363_2","argument":"車や公共交通機関を積極的に利用することが推奨される。","x":-5.0134244,"y":10.962368,"p":0,"cluster_ids":["0","1_8","2_45"],"attributes":null,"url":null},{"arg_id":"Acsv-363_3","argument":"赤字になりがちな稲作や土地の整備を行うことが必要である。","x":-5.6682754,"y":11.7591715,"p":0,"cluster_ids":["0","1_6","2_55"],"attributes":null,"url":null},{"arg_id":"Acsv-365_0","argument":"障害のある人に対する偏見を持たないように、小さい頃から共に学習すべき","x":-6.1638393,"y":9.493099,"p":0,"cluster_ids":["0","1_6","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-365_1","argument":"障害のある人が身近にいることが当たり前であるという認識をすべての市民が持つべき","x":-6.085117,"y":9.996697,"p":0,"cluster_ids":["0","1_6","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-367_0","argument":"障害児も含め、どんな子どもでも挨拶ができる街であるべき","x":-6.369239,"y":9.72614,"p":0,"cluster_ids":["0","1_6","2_52"],"attributes":null,"url":null},{"arg_id":"Acsv-367_1","argument":"偏見なく健やかに育つ環境が重要である","x":-5.9695997,"y":9.26014,"p":0,"cluster_ids":["0","1_6","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-368_0","argument":"多様な人が互いに尊重するためには、個々人が豊かな感性と教養を持つ必要がある。","x":-5.89658,"y":7.9733596,"p":0,"cluster_ids":["0","1_2","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-368_1","argument":"ほんとうの意味で賢い市民になりたい。","x":-5.9498396,"y":10.965151,"p":0,"cluster_ids":["0","1_6","2_57"],"attributes":null,"url":null},{"arg_id":"Acsv-368_2","argument":"そういう人が増えていくまちでありたい。","x":-5.047682,"y":11.475008,"p":0,"cluster_ids":["0","1_8","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-369_0","argument":"多様な人が年齢関係なくお互いを尊重し、繋がり、助け合える社会が必要である。","x":-6.188089,"y":8.039514,"p":0,"cluster_ids":["0","1_2","2_59"],"attributes":null,"url":null},{"arg_id":"Acsv-370_0","argument":"若い世代が増えないと衰退してしまう。","x":-3.9708784,"y":11.138318,"p":0,"cluster_ids":["0","1_8","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-370_1","argument":"若い世代が生き生きと自分らしく生きていける場所が必要である。","x":-3.945959,"y":10.875189,"p":0,"cluster_ids":["0","1_8","2_39"],"attributes":null,"url":null},{"arg_id":"Acsv-374_0","argument":"老若男女が役割を持ち、活気をもって暮らす姿が望ましい。","x":-3.547012,"y":10.798633,"p":0,"cluster_ids":["0","1_8","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-415_0","argument":"自分のふるさとを大切にするべきである。","x":-5.023055,"y":8.742693,"p":0,"cluster_ids":["0","1_1","2_28"],"attributes":null,"url":null},{"arg_id":"Acsv-416_0","argument":"みんなの居場所を作るべきである","x":-5.0605836,"y":10.266455,"p":0,"cluster_ids":["0","1_4","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-416_1","argument":"いろんな人を笑顔にできることが重要である","x":-3.5147607,"y":8.359982,"p":0,"cluster_ids":["0","1_5","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-417_0","argument":"自分のやりたいことを楽しくできている","x":-4.579522,"y":9.740242,"p":0,"cluster_ids":["0","1_4","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-418_0","argument":"舞鶴の良さを教えられる人でありたい","x":-2.7310777,"y":12.091703,"p":0,"cluster_ids":["0","1_7","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-419_0","argument":"たくさんの人が観光スポットに行きたいと思えるようにするべきである。","x":-5.397887,"y":10.95447,"p":0,"cluster_ids":["0","1_6","2_35"],"attributes":null,"url":null},{"arg_id":"Acsv-420_0","argument":"人に優しくするべきである","x":-4.264119,"y":8.238243,"p":0,"cluster_ids":["0","1_3","2_32"],"attributes":null,"url":null},{"arg_id":"Acsv-420_1","argument":"街が好きであり、幸せでありたい","x":-6.213039,"y":11.169015,"p":0,"cluster_ids":["0","1_6","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-432_0","argument":"他人のためになることをたまにやりたい","x":-4.0404334,"y":8.31933,"p":0,"cluster_ids":["0","1_3","2_32"],"attributes":null,"url":null},{"arg_id":"Acsv-434_0","argument":"自然を大切にする世の中が必要である。","x":-5.5877748,"y":8.567498,"p":0,"cluster_ids":["0","1_1","2_37"],"attributes":null,"url":null},{"arg_id":"Acsv-435_0","argument":"子どもの人権尊重を起点として、あらゆる人々の尊厳が守られる社会であるべき。","x":-6.46911,"y":8.442288,"p":0,"cluster_ids":["0","1_2","2_50"],"attributes":null,"url":null},{"arg_id":"Acsv-435_1","argument":"自分の頭で考え、対話しつつ試行錯誤できる市民が増えることが重要。","x":-5.6285534,"y":10.740364,"p":0,"cluster_ids":["0","1_6","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-435_2","argument":"頼もしい若者たちが新しい実験を繰り返し、それを見守る大人たちも共に成長する社会が望ましい。","x":-6.339459,"y":8.637064,"p":0,"cluster_ids":["0","1_2","2_50"],"attributes":null,"url":null},{"arg_id":"Acsv-435_3","argument":"生命力に満ちた社会であることが重要。","x":-6.0221486,"y":8.387701,"p":0,"cluster_ids":["0","1_2","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-441_0","argument":"失敗しても大丈夫な世の中であるべき","x":-3.7256544,"y":7.531427,"p":0,"cluster_ids":["0","1_3","2_25"],"attributes":null,"url":null},{"arg_id":"Acsv-442_0","argument":"ヤンキーを減らしたい","x":-3.680941,"y":7.8824387,"p":0,"cluster_ids":["0","1_3","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-442_1","argument":"誰もが優しく接することができる社会になりたい","x":-6.6169467,"y":7.9238515,"p":0,"cluster_ids":["0","1_2","2_51"],"attributes":null,"url":null},{"arg_id":"Acsv-443_0","argument":"便利で不満の少ない街を目指すべきである。","x":-6.157356,"y":11.352854,"p":0,"cluster_ids":["0","1_6","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-446_0","argument":"尊重、尊敬できあう町を目指すべきである。","x":-6.2715487,"y":11.064788,"p":0,"cluster_ids":["0","1_6","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-447_0","argument":"自由に生きることが重要である","x":-5.3104205,"y":9.313863,"p":0,"cluster_ids":["0","1_4","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-449_0","argument":"かねを出すべきである","x":-3.6962323,"y":8.000117,"p":0,"cluster_ids":["0","1_3","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-450_0","argument":"暮らしやすい街を作ることが重要である。","x":-5.97462,"y":11.575242,"p":0,"cluster_ids":["0","1_6","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-451_0","argument":"多様な人がお互いを尊重し自由に生きられる街を目指すべき","x":-6.694437,"y":10.2288885,"p":0,"cluster_ids":["0","1_6","2_26"],"attributes":null,"url":null},{"arg_id":"Acsv-452_0","argument":"ゴミをできるだけ減らして、SDGsに協力すべき","x":-4.6921563,"y":11.127673,"p":0,"cluster_ids":["0","1_8","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-453_0","argument":"一人一人がみんなを思い合う世の中が望ましい。","x":-4.9403777,"y":7.545923,"p":0,"cluster_ids":["0","1_3","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-454_0","argument":"全員が正しい行動ができるべきである。","x":-4.513617,"y":9.225315,"p":0,"cluster_ids":["0","1_5","2_40"],"attributes":null,"url":null},{"arg_id":"Acsv-455_0","argument":"厳しいことがない","x":-4.355394,"y":7.660246,"p":0,"cluster_ids":["0","1_3","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-456_0","argument":"まわりの人たちと協力できる優しい姿が重要である。","x":-4.740107,"y":8.264669,"p":0,"cluster_ids":["0","1_1","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-459_0","argument":"自由に生きられるべきである","x":-5.206923,"y":9.472888,"p":0,"cluster_ids":["0","1_4","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-460_0","argument":"お互いを気遣う街であるべき","x":-6.6331973,"y":10.528826,"p":0,"cluster_ids":["0","1_6","2_26"],"attributes":null,"url":null},{"arg_id":"Acsv-460_1","argument":"自由に生きていける街であるべき","x":-6.2227755,"y":10.554806,"p":0,"cluster_ids":["0","1_6","2_27"],"attributes":null,"url":null},{"arg_id":"Acsv-461_0","argument":"助け合う地域が重要である","x":-4.822818,"y":8.772198,"p":0,"cluster_ids":["0","1_1","2_28"],"attributes":null,"url":null},{"arg_id":"Acsv-463_0","argument":"悲しんだり嫌な思いをする人がいない世の中が望ましい。","x":-4.680182,"y":7.251066,"p":0,"cluster_ids":["0","1_3","2_34"],"attributes":null,"url":null},{"arg_id":"Acsv-465_0","argument":"みんなが楽しく過ごすことが重要であり、困ることがない状況を目指すべきである。","x":-3.651708,"y":9.385959,"p":0,"cluster_ids":["0","1_5","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-467_0","argument":"楽しい生活ができる","x":-4.238729,"y":9.853017,"p":0,"cluster_ids":["0","1_4","2_56"],"attributes":null,"url":null},{"arg_id":"Acsv-468_0","argument":"みんなが笑っていられる姿が大切である。","x":-3.532151,"y":8.640752,"p":0,"cluster_ids":["0","1_5","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-469_0","argument":"どの年代でも楽しく生きられる","x":-3.9063947,"y":10.32971,"p":0,"cluster_ids":["0","1_8","2_46"],"attributes":null,"url":null},{"arg_id":"Acsv-470_0","argument":"若い人からお年寄りまで、いろんな人が楽しく過ごすべきである。","x":-3.644602,"y":10.565097,"p":0,"cluster_ids":["0","1_8","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-472_0","argument":"赤ちゃんからお年寄りまで全員が元気なまちを目指すべき","x":-4.0799193,"y":10.768475,"p":0,"cluster_ids":["0","1_8","2_39"],"attributes":null,"url":null},{"arg_id":"Acsv-473_0","argument":"笑顔で過ごしていたい。","x":-3.200978,"y":8.873515,"p":0,"cluster_ids":["0","1_5","2_49"],"attributes":null,"url":null},{"arg_id":"Acsv-473_1","argument":"過ごしやすい舞鶴に住みたい。","x":-3.126237,"y":11.702366,"p":0,"cluster_ids":["0","1_7","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-473_2","argument":"人と人との繋がりが沢山あることが重要。","x":-5.134557,"y":8.079826,"p":0,"cluster_ids":["0","1_1","2_31"],"attributes":null,"url":null},{"arg_id":"Acsv-474_0","argument":"家族全員で安心して過ごしている姿が重要である","x":-3.4099574,"y":9.574295,"p":0,"cluster_ids":["0","1_5","2_43"],"attributes":null,"url":null},{"arg_id":"Acsv-475_0","argument":"差別のない誰もが尊重し合える社会を目指すべきである。","x":-6.825538,"y":8.449453,"p":0,"cluster_ids":["0","1_2","2_33"],"attributes":null,"url":null},{"arg_id":"Acsv-477_0","argument":"誰もが失敗を笑わずに仲良くしている姿が望ましい。","x":-3.3932514,"y":7.643436,"p":0,"cluster_ids":["0","1_3","2_61"],"attributes":null,"url":null},{"arg_id":"Acsv-478_0","argument":"みんなが協力している姿は重要である。","x":-4.4361577,"y":8.926454,"p":0,"cluster_ids":["0","1_5","2_60"],"attributes":null,"url":null},{"arg_id":"Acsv-479_0","argument":"みんなに好かれる姿を持つことが重要である。","x":-4.156869,"y":8.784207,"p":0,"cluster_ids":["0","1_5","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-480_0","argument":"多様な人がお互いを尊重し自由に生きられる社会が必要である。","x":-6.1866455,"y":8.336547,"p":0,"cluster_ids":["0","1_2","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-512_0","argument":"賑わっている街は活気があり、人々が集まる場所である。","x":-6.56739,"y":11.000738,"p":0,"cluster_ids":["0","1_6","2_54"],"attributes":null,"url":null},{"arg_id":"Acsv-517_0","argument":"多様な人が認め合い、助け合いながらのんびりと生活できる世の中が望ましい。","x":-5.540064,"y":7.526698,"p":0,"cluster_ids":["0","1_3","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-518_0","argument":"高校を卒業した若者が進学先で地元が好きだと言えるまちであるべき","x":-4.7240167,"y":11.704229,"p":0,"cluster_ids":["0","1_8","2_53"],"attributes":null,"url":null},{"arg_id":"Acsv-518_1","argument":"若者が自慢できるまちを作る必要がある","x":-4.480296,"y":11.449239,"p":0,"cluster_ids":["0","1_8","2_44"],"attributes":null,"url":null}],"clusters":[{"level":0,"id":"0","label":"全体","takeaway":"","value":99,"parent":"","density_rank_percentile":0},{"level":1,"id":"1_7","label":"舞鶴市の地域活性化と住環境の向上に向けた取り組み","takeaway":"舞鶴市における住環境の改善や地域社会の活性化に対する期待が高まっています。市民は、住みやすい街づくりや就職機会の増加を求めており、新たにオープンしたスターバックスでの勤務希望もその一環です。また、舞鶴の魅力を広める活動を通じて地域愛を育むことが重要視されており、地域の良さを伝える人材の育成が求められています。これにより、舞鶴市が「人を大事にできる」地域として発展することが期待されています。","value":7,"parent":"0","density_rank_percentile":0.125},{"level":1,"id":"1_4","label":"健康的で自由なライフスタイルの実現とコミュニティの重要性","takeaway":"個々の健康を最優先にし、自己肯定感を高めることで、より良い人間関係や生活の質を享受することが期待されています。また、各人が自分の特性に基づいた自由な生き方を追求し、自己選択の権利が尊重される社会の必要性が強調されています。さらに、地域におけるコミュニティスペースの重要性が認識され、人々が集まり交流できる居場所を作ることで、相互理解や支援が促進されることが求められています。","value":10,"parent":"0","density_rank_percentile":0.5},{"level":1,"id":"1_1","label":"地域の絆を深める自立した生活と持続可能な環境の実現","takeaway":"地域活動において多様な人材が繋がり、互いに助け合うことで、より豊かなコミュニティが形成されることが求められています。また、個々が自立した生き方を追求し、他者を尊重しつつ協力し合う姿勢が重要です。さらに、自然環境を大切にし、持続可能な社会を築くことが未来の世代に対する責任であると強調されています。地域の絆を深め、自給自足の生活を実現することで、地域全体の発展と環境保護が両立することが期待されています。","value":9,"parent":"0","density_rank_percentile":0.375},{"level":1,"id":"1_3","label":"共感と支え合いによる幸福な社会の実現","takeaway":"多様な人々が互いに認め合い、助け合いながら、失敗を恐れずに挑戦できる環境を整えることが求められています。個々が他者を思いやり、優しさを持って接することで、誰もが幸せを感じられる社会の構築が目指されています。また、過度な我慢を強いる文化に対する反発もあり、より公平で理解のある社会を実現するための具体的な施策が必要とされています。","value":14,"parent":"0","density_rank_percentile":0.75},{"level":1,"id":"1_2","label":"共生と尊重に基づく平和で安全な社会の実現","takeaway":"すべての人が尊重され、安心して暮らせる社会を目指すためには、犯罪率の低下や差別の排除が不可欠です。また、年齢や背景に関係なく多様な人々が互いに助け合い、共存する姿勢が求められています。子どもの人権を尊重し、豊かな感性と教養を持つことが、個々の尊厳を守る基盤となります。さらに、優しさや共感が根付いた社会を築くことで、より良いコミュニティの形成が期待されます。","value":13,"parent":"0","density_rank_percentile":0.25},{"level":1,"id":"1_5","label":"地域社会における安心・快適な生活環境の構築","takeaway":"地域社会において、子どもたちが安全に過ごせる環境や、家族全員が安心して生活できる医療サービスの充実が求められています。また、笑顔を通じたコミュニケーションや協力によるチームワークが、社会的なつながりや幸福感を高める要素として重要視されています。全員が正しい行動を取り、ポジティブな生活を追求することで、みんなが楽しく過ごせる状況を目指す姿勢が強調されています。","value":11,"parent":"0","density_rank_percentile":0.625},{"level":1,"id":"1_8","label":"世代を超えた活力ある持続可能なまちづくりの推進","takeaway":"すべての世代が安心して楽しく暮らせる環境を整えることが求められています。高齢者が自由に移動できるインフラの整備や、若者が自分の住む地域に誇りを持てるような魅力的なまちづくりが重要です。また、持続可能な開発目標（SDGs）を意識し、廃棄物削減や環境保護に取り組むことで、地域社会全体の活性化を図ることが期待されています。世代間の調和を重視し、赤ちゃんからお年寄りまでが元気に過ごせるまちを目指すことが、地域の持続可能性に寄与するでしょう。","value":15,"parent":"0","density_rank_percentile":0.875},{"level":1,"id":"1_6","label":"市民の幸福と共生を実現する住みやすい街づくり","takeaway":"市民が快適に生活できる環境を整え、互いに尊重し合う関係を築くことが重要です。住みやすさや利便性を追求し、障害のある人々を含むすべての市民が自由に生活できる社会を目指します。また、地域の活性化や観光スポットの魅力向上を図ることで、賑わいのある街を実現し、定住を促進するための具体的な施策が求められています。","value":20,"parent":"0","density_rank_percentile":1},{"level":2,"id":"2_22","label":"舞鶴市の住環境と地域社会の向上への期待","takeaway":"この意見グループは、舞鶴市に住むことの利点や、地域社会の人々が互いに支え合う環境の重要性に焦点を当てています。また、舞鶴市内での就職機会の増加を通じて、住みやすい街づくりや地域の活性化を望む声が中心となっています。","value":4,"parent":"1_7","density_rank_percentile":0.96875},{"level":2,"id":"2_41","label":"健康を最優先するライフスタイル","takeaway":"この意見は、健康を最も重要視する考え方を反映しています。健康を第一に考えることで、日常生活や選択において健康的な選択を優先する姿勢が示されています。","value":1,"parent":"1_4","density_rank_percentile":0.015625},{"level":2,"id":"2_62","label":"自己肯定感の向上とポジティブな自己認識","takeaway":"この意見グループは、自己肯定感が高いことに関連するポジティブな感情や自己認識の重要性に焦点を当てています。自己肯定感が高い人々は、自分自身を受け入れ、価値を感じることで、より良い人間関係や生活の質を享受しているという点が共通しています。","value":1,"parent":"1_4","density_rank_percentile":0.03125},{"level":2,"id":"2_31","label":"地域活動における多様な人材の繋がりの重要性","takeaway":"この意見グループは、地域活動において人と人との繋がりが重要であり、多様なバックグラウンドを持つ人材が関わることで、より豊かなコミュニティが形成されるべきだという考えが中心です。地域の活性化や相互理解を促進するためには、様々な人々が協力し合うことが不可欠であるという視点が強調されています。","value":2,"parent":"1_1","density_rank_percentile":0.734375},{"level":2,"id":"2_6","label":"現実逃避と不安の回避","takeaway":"この意見グループは、厳しい状況や困難な事態に対する想像力の欠如や、それに伴う不安を避ける傾向が見受けられます。厳しいことがないという表現は、現実からの逃避や、ポジティブな思考を維持しようとする姿勢を反映しています。","value":2,"parent":"1_3","density_rank_percentile":0.578125},{"level":2,"id":"2_10","label":"平和な社会の実現に向けた意識の高まり","takeaway":"この意見は、争いの少ない社会を目指すことの重要性を強調しており、平和や調和を重視する価値観が反映されています。人々が争いを避け、共存を目指す姿勢が求められていることが示されています。","value":1,"parent":"1_2","density_rank_percentile":0.046875},{"level":2,"id":"2_33","label":"平等と安全を重視した社会の実現","takeaway":"この意見グループは、差別のない社会を目指すことと、犯罪率の低い安全な社会の実現が重要であるという共通のテーマを持っています。両者は、すべての人が尊重され、安心して暮らせる環境を作ることに寄与するという点で結びついています。","value":2,"parent":"1_2","density_rank_percentile":0.8125},{"level":2,"id":"2_21","label":"共生と助け合いの豊かな社会","takeaway":"この意見グループは、多様な人々が互いに認め合い、助け合いながら、のんびりとした生活を送ることの重要性を強調しています。また、娯楽が豊富に存在する中で、個々が他者を思いやる心を持つことが、理想的な社会の実現に寄与するという視点が中心です。","value":3,"parent":"1_3","density_rank_percentile":1},{"level":2,"id":"2_25","label":"失敗を受容する社会の構築","takeaway":"この意見グループは、失敗を恐れずに挑戦できる環境の重要性を強調しています。失敗を許容し、再挑戦を支援する姿勢が求められており、個人の成長や社会全体の発展に寄与することが期待されています。","value":2,"parent":"1_3","density_rank_percentile":0.5625},{"level":2,"id":"2_30","label":"新店舗でのスターバックス勤務希望","takeaway":"この意見は、新しくオープンしたスターバックスでの勤務を希望する声を反映しています。新しい環境での仕事に対する期待感や、スターバックスというブランドへの関心が強調されています。","value":1,"parent":"1_7","density_rank_percentile":0.0625},{"level":2,"id":"2_59","label":"多様性と相互尊重による共生社会の実現","takeaway":"この意見グループは、年齢や背景に関係なく多様な人々が互いに尊重し合い、助け合うことができる社会の重要性を強調しています。多様性を受け入れ、共に生きることで、より良い社会を築く必要性が語られています。","value":2,"parent":"1_2","density_rank_percentile":0.828125},{"level":2,"id":"2_9","label":"子どもたちの安全と快適な環境の確保","takeaway":"この意見グループは、子どもたちが安心して過ごせる環境を重視しており、周囲の大人たちがそのために配慮し、迷惑をかけないようにすることが重要であるという考えが中心です。また、みんなが楽しく過ごせる状況を目指し、不安のない日々を求める姿勢が表れています。","value":3,"parent":"1_5","density_rank_percentile":0.796875},{"level":2,"id":"2_3","label":"多文化共生の否定とその影響","takeaway":"この意見は、多文化共生の概念に対する否定的な見解を示しており、その結果として社会に与える影響や問題点についての懸念が表れています。多文化共生を終わらせるべきという主張は、文化的な対立や摩擦を避けるための手段として捉えられている可能性があります。","value":1,"parent":"1_2","density_rank_percentile":0.078125},{"level":2,"id":"2_48","label":"日本人への不当な我慢の強要に対する反発","takeaway":"この意見は、日本人に対して過度な我慢や忍耐を求めることが不適切であるという主張を中心に展開されています。社会的な圧力や文化的な期待が個人に与える影響についての懸念が表明されており、より公平で理解のある社会を求める声が反映されています。","value":1,"parent":"1_3","density_rank_percentile":0.09375},{"level":2,"id":"2_7","label":"個々の特性に基づく自由な生き方の重要性","takeaway":"この意見グループは、各個人が自分の特性や価値観に応じた生き方を追求することの重要性を強調しています。自由に生きることが個人の幸福や自己実現に繋がるという考えが中心にあり、個々の自由を尊重する社会の必要性が示されています。","value":3,"parent":"1_4","density_rank_percentile":0.84375},{"level":2,"id":"2_12","label":"他者を尊重しつつ自立した生き方の重要性","takeaway":"この意見グループは、他者を馬鹿にすることなく、相手に依存しない生き方の重要性を強調しています。また、周囲の人々と協力し合う優しさや共感の姿勢が大切であるという点も含まれており、自己の独立性と他者との調和を両立させることが求められています。","value":2,"parent":"1_1","density_rank_percentile":0.59375},{"level":2,"id":"2_47","label":"自立した生活と自給自足の重要性","takeaway":"この意見グループは、個々が自立した生き方を追求することの重要性と、特に光熱水の自給自足が生活の質を向上させる要素として強調されています。自立した生活は、経済的な安定や環境への配慮を含む広範な概念であり、持続可能な生活スタイルの実現に向けた具体的なアプローチが示されています。","value":2,"parent":"1_1","density_rank_percentile":0.859375},{"level":2,"id":"2_29","label":"高齢者の移動自由を支えるインフラ整備","takeaway":"この意見は、高齢者が車を運転できない状況でも、自由に街を行き来できるような環境の重要性を強調しています。高齢者の移動手段の確保や、公共交通機関の充実、バリアフリーなインフラの整備が求められていることが示されています。","value":1,"parent":"1_8","density_rank_percentile":0.109375},{"level":2,"id":"2_43","label":"地域の医療と家族の安心感の重要性","takeaway":"この意見グループは、地域における医療の信頼性と、家族全員が安心して生活できる環境の重要性に焦点を当てています。医療サービスが充実し、地域住民が安心して医療を受けられることが、家族の健康と幸福に直結するという考えが表れています。","value":2,"parent":"1_5","density_rank_percentile":0.671875},{"level":2,"id":"2_5","label":"舞鶴の魅力を伝える活動と地域愛の育成","takeaway":"この意見グループは、舞鶴の良さを他者に伝えることを重視し、舞鶴海洋少年団での活動を通じて地域の魅力を広めることに対する強い意欲が表れています。また、舞鶴での育ちを誇りに思い、地域への愛着を育むことが重要視されています。","value":2,"parent":"1_7","density_rank_percentile":0.703125},{"level":2,"id":"2_36","label":"市民の定住促進と住みやすいまちづくり","takeaway":"この意見グループは、市民が快適に生活できる環境を整えることの重要性に焦点を当てています。市民が住みやすいと感じるまちを作ることで、地域からの流出を防ぎ、定住を促進することが求められています。","value":2,"parent":"1_6","density_rank_percentile":0.921875},{"level":2,"id":"2_63","label":"持続可能な都市開発の重要性","takeaway":"この意見は、持続可能なまちづくりの必要性に焦点を当てており、環境保護や資源の効率的な利用、地域社会の活性化など、持続可能性を考慮した都市開発の重要性を強調しています。","value":1,"parent":"1_8","density_rank_percentile":0.125},{"level":2,"id":"2_15","label":"若者の夢と未来を支える社会の構築","takeaway":"この意見グループは、若者が将来に対して夢を持てるような環境を整えることの重要性を強調しています。また、若い世代の増加が社会の持続可能性に直結しているという認識があり、若者の育成や支援が社会全体の発展に寄与するという前向きな視点が示されています。","value":2,"parent":"1_8","density_rank_percentile":0.75},{"level":2,"id":"2_2","label":"世代共生による活気ある社会の実現","takeaway":"この意見グループは、老若男女がそれぞれの役割を持ちながら、安心して楽しく暮らすことが重要であるという考えに基づいています。特に年配者が安心して生活できる環境を整えることが、全世代にとっての活力を生む社会の実現に寄与するという視点が強調されています。","value":3,"parent":"1_8","density_rank_percentile":0.984375},{"level":2,"id":"2_11","label":"多様性の尊重と個人の教養の重要性","takeaway":"この意見グループは、多様な人々が互いに尊重し合うためには、個々人が豊かな感性と教養を持つことが不可欠であるという考えに基づいています。また、多様性の中で個人がその人らしく生きることが、守られるべき尊厳であるという視点が強調されています。","value":2,"parent":"1_2","density_rank_percentile":0.6875},{"level":2,"id":"2_42","label":"自己選択と自己実現の権利の尊重","takeaway":"この意見は、障害や病気、高齢にかかわらず、個人が自分の選択を尊重され、自分でできることを実現する権利が重要であるという考えを示しています。個々のニーズや希望に基づいた選択肢の提供が求められ、自己決定権の重要性が強調されています。","value":1,"parent":"1_4","density_rank_percentile":0.140625},{"level":2,"id":"2_58","label":"人を大切にする町づくりと争いのない社会","takeaway":"この意見は、人々を大事にすることが町の発展に寄与し、争いや派閥の問題を解消する可能性について述べています。人間関係の重視が、より平和で調和の取れた社会を実現するための鍵であるという前向きな視点が示されています。","value":1,"parent":"1_6","density_rank_percentile":0.15625},{"level":2,"id":"2_14","label":"市民の主体性と対話を重視した地域活性化","takeaway":"この意見グループは、地域社会において市民が自らの得意分野を活かし、主体的に考え、対話を通じて試行錯誤を行うことの重要性を強調しています。市民の参加と協力によって、より活気ある町づくりが実現されるという前向きなビジョンが示されています。","value":2,"parent":"1_6","density_rank_percentile":0.875},{"level":2,"id":"2_38","label":"投資利益の享受と生活の質向上","takeaway":"この意見は、投資によって得た利益を活用して、通販や外食、映画鑑賞などの娯楽活動を楽しむことが重要であるという考えを示しています。投資の成果を生活の質を向上させるために使うべきだという前向きな姿勢が反映されています。","value":1,"parent":"1_4","density_rank_percentile":0.171875},{"level":2,"id":"2_45","label":"持続可能な移動手段の推進","takeaway":"この意見グループは、環境への配慮や交通渋滞の緩和を目的として、車や公共交通機関の利用を促進することの重要性を強調しています。持続可能な移動手段を選ぶことで、個人の利便性だけでなく、社会全体の交通システムの効率化にも寄与することが期待されています。","value":1,"parent":"1_8","density_rank_percentile":0.1875},{"level":2,"id":"2_55","label":"稲作の持続可能性と土地整備の重要性","takeaway":"この意見は、稲作が赤字になりやすい現状を踏まえ、持続可能な農業を実現するためには土地の整備が不可欠であるという認識を示しています。稲作の経済的な安定を図るための具体的な施策として、土地の整備が重要であることが強調されています。","value":1,"parent":"1_6","density_rank_percentile":0.203125},{"level":2,"id":"2_18","label":"障害者に対する理解促進と共生教育の重要性","takeaway":"この意見グループは、障害のある人々に対する偏見をなくすためには、幼少期からの共生教育が不可欠であるという考えに基づいています。偏見のない健やかな育成環境を整えることが、社会全体の理解を深め、共生を促進するための重要な要素であることが強調されています。","value":2,"parent":"1_6","density_rank_percentile":0.9375},{"level":2,"id":"2_0","label":"障害者の存在を受け入れる社会の実現","takeaway":"この意見グループは、障害のある人々が身近にいることを当たり前とする認識を全ての市民が持つべきであるという考えを中心に展開されています。障害者の存在を受け入れ、共生社会を築くことの重要性が強調されており、社会全体の意識改革が求められています。","value":1,"parent":"1_6","density_rank_percentile":0.21875},{"level":2,"id":"2_52","label":"すべての子どもが挨拶できる共生社会の実現","takeaway":"この意見グループは、障害の有無にかかわらず、すべての子どもが挨拶を交わすことができる環境の重要性を強調しています。挨拶はコミュニケーションの基本であり、子どもたちが社会に参加し、互いに理解し合うための第一歩として位置づけられています。","value":1,"parent":"1_6","density_rank_percentile":0.234375},{"level":2,"id":"2_57","label":"市民の知識と意識の向上","takeaway":"この意見は、市民がより賢くなることを目指しており、知識や意識を高めることの重要性を強調しています。市民としての責任を果たし、社会に貢献するための学びや成長を求める姿勢が表れています。","value":1,"parent":"1_6","density_rank_percentile":0.25},{"level":2,"id":"2_4","label":"共生と多様性を重視した未来志向のまちづくり","takeaway":"この意見は、地域社会において多様な人々が共に生活し、互いに支え合うことを重視する姿勢を示しています。人々が増えていくことに対する期待感があり、共生の価値を大切にしたまちづくりのビジョンが反映されています。","value":1,"parent":"1_8","density_rank_percentile":0.265625},{"level":2,"id":"2_39","label":"世代を超えた活力あるコミュニティの創造","takeaway":"この意見グループは、赤ちゃんからお年寄りまで全ての世代が元気で活力に満ちた生活を送ることができるまちづくりの重要性を強調しています。また、特に若い世代が自分らしく生き生きと過ごせる環境の必要性についても言及されており、世代間の調和と活力を重視したコミュニティの形成が求められています。","value":2,"parent":"1_8","density_rank_percentile":0.640625},{"level":2,"id":"2_28","label":"地域コミュニティの重要性とふるさとの価値","takeaway":"この意見グループは、個人が自分のふるさとを大切にし、地域社会の助け合いの重要性を強調しています。地域コミュニティの絆や支援が、個々の生活や地域全体の発展に寄与するという考えが中心となっています。","value":2,"parent":"1_1","density_rank_percentile":0.71875},{"level":2,"id":"2_24","label":"コミュニティスペースの重要性と必要性","takeaway":"この意見は、地域やグループにおいて人々が集まり、交流できる居場所の必要性を強調しています。居場所を作ることで、コミュニティの絆が深まり、相互理解や支援が促進されるという前向きな視点が示されています。","value":1,"parent":"1_4","density_rank_percentile":0.28125},{"level":2,"id":"2_20","label":"笑顔を通じたコミュニケーションの重要性","takeaway":"この意見グループは、笑顔や笑いが人々の心をつなぎ、コミュニケーションを円滑にする重要な要素であることを強調しています。多様な人々を笑顔にすることが、社会的なつながりや幸福感を高めるために不可欠であるという視点が中心です。","value":2,"parent":"1_5","density_rank_percentile":0.890625},{"level":2,"id":"2_16","label":"自己実現と楽しさの追求","takeaway":"この意見は、自分のやりたいことを楽しみながら実現できているというポジティブな感情を表現しています。個人の目標や趣味に対する満足感が強調されており、自己実現の重要性や楽しさを重視する姿勢が見受けられます。","value":1,"parent":"1_4","density_rank_percentile":0.296875},{"level":2,"id":"2_35","label":"観光スポットの魅力向上と集客戦略","takeaway":"この意見は、観光スポットの魅力を高めることが重要であり、多くの人々が訪れたくなるような施策や戦略を講じるべきだという考えを示しています。観光地の活性化や集客を促進するための具体的な取り組みが求められています。","value":1,"parent":"1_6","density_rank_percentile":0.3125},{"level":2,"id":"2_32","label":"他者への思いやりと優しさの重要性","takeaway":"この意見グループは、他人のために行動することや、人に対して優しく接することの重要性を強調しています。自己中心的な行動から脱却し、社会的なつながりや共感を大切にする姿勢が見受けられ、他者への貢献を通じて自己成長を図る意識が反映されています。","value":2,"parent":"1_3","density_rank_percentile":0.78125},{"level":2,"id":"2_8","label":"幸せで尊重し合うコミュニティの形成","takeaway":"この意見グループは、街に対する愛着や幸福感を強調し、互いに尊重し合う関係を築くことの重要性を訴えています。コミュニティの中での相互尊重が、より良い社会を作るための基盤であるという考えが中心にあります。","value":2,"parent":"1_6","density_rank_percentile":0.625},{"level":2,"id":"2_37","label":"持続可能な自然環境の重要性","takeaway":"この意見は、自然環境を保護し、持続可能な社会を築くことの重要性を強調しています。自然を大切にすることで、未来の世代に豊かな環境を残す必要性が訴えられており、環境保護の意識を高めることが求められています。","value":1,"parent":"1_1","density_rank_percentile":0.328125},{"level":2,"id":"2_50","label":"子どもの人権と共生を重視した社会の実現","takeaway":"この意見グループは、子どもの人権を尊重することが、すべての人々の尊厳を守る基盤であると考えています。また、若者たちが新しい挑戦を行い、それを支える大人たちとの共成長を通じて、より良い社会を築くことが重要であるという視点が強調されています。","value":2,"parent":"1_2","density_rank_percentile":0.765625},{"level":2,"id":"2_19","label":"多様性と尊重に基づく生命力ある社会の実現","takeaway":"この意見グループは、生命力に満ちた社会の重要性を強調し、多様な人々が互いに尊重し合い、自由に生きることができる社会の必要性に焦点を当てています。多様性の受容と相互尊重が、社会全体の活力を高める要素であるという考えが中心です。","value":2,"parent":"1_2","density_rank_percentile":0.65625},{"level":2,"id":"2_13","label":"ヤンキー対策のための資金投入","takeaway":"この意見グループは、ヤンキーの数を減少させるためには、具体的な資金を投入する必要があるという考えに基づいています。ヤンキーの問題解決には、経済的な支援や施策が重要であるという点が強調されています。","value":2,"parent":"1_3","density_rank_percentile":0.609375},{"level":2,"id":"2_51","label":"優しさと共感に満ちた社会の実現","takeaway":"この意見は、全ての人が優しく接することができる社会を目指すという願望を表しています。優しさや共感が根付いた社会は、個々の人間関係を豊かにし、より良いコミュニティを形成することができるという前向きなビジョンが中心です。","value":1,"parent":"1_2","density_rank_percentile":0.34375},{"level":2,"id":"2_17","label":"住みやすさと利便性を追求した街づくり","takeaway":"この意見グループは、暮らしやすさを重視した街づくりの重要性を強調しており、便利で不満の少ない環境を目指すべきだという考えが中心です。住民の生活の質を向上させるための具体的な施策や理念が求められています。","value":2,"parent":"1_6","density_rank_percentile":0.90625},{"level":2,"id":"2_26","label":"相互尊重と共生を重視した理想の街づくり","takeaway":"この意見グループは、多様な人々が互いに尊重し合い、自由に生活できる環境を目指すことに焦点を当てています。また、住民同士の気遣いや配慮が重要であるという考えが強調されており、共生の精神を基盤とした街づくりの必要性が示されています。","value":2,"parent":"1_6","density_rank_percentile":0.953125},{"level":2,"id":"2_23","label":"SDGsへの貢献と廃棄物削減の重要性","takeaway":"この意見グループは、持続可能な開発目標（SDGs）に対する意識の高まりと、環境保護の観点からゴミを減らすことの重要性を強調しています。廃棄物削減がSDGsの達成に寄与するという考えが中心にあり、環境への配慮が求められています。","value":1,"parent":"1_8","density_rank_percentile":0.359375},{"level":2,"id":"2_40","label":"全員の行動規範の重要性","takeaway":"この意見は、全ての人が正しい行動を取ることの重要性を強調しています。社会や組織において、個々の行動が全体に影響を与えるため、全員が正しい行動をすることが求められるという考えが中心です。","value":1,"parent":"1_5","density_rank_percentile":0.375},{"level":2,"id":"2_27","label":"自由な生活を実現する街づくり","takeaway":"この意見は、住民が自由に生きることができる環境を求めるものであり、街の設計や政策が個人の自由を尊重し、促進することが重要であるという考えが中心です。","value":1,"parent":"1_6","density_rank_percentile":0.390625},{"level":2,"id":"2_34","label":"誰もが幸せを感じる社会の実現","takeaway":"この意見は、悲しみや嫌な思いをする人がいない理想的な社会を求めるものであり、全ての人が幸福を感じられる環境の重要性を強調しています。人々の感情や幸福に配慮した社会の構築を目指す姿勢が表れています。","value":1,"parent":"1_3","density_rank_percentile":0.40625},{"level":2,"id":"2_56","label":"充実した楽しい生活の実現","takeaway":"この意見は、生活の質を向上させることに対する期待や願望を表現しています。楽しい生活を送るための要素や条件についての考えが含まれており、個人の幸福感や満足度を高めることに焦点を当てています。","value":1,"parent":"1_4","density_rank_percentile":0.421875},{"level":2,"id":"2_46","label":"年代を超えた楽しい生活の実現","takeaway":"この意見は、年齢に関係なく、どの年代でも楽しく生きることができるというポジティブな視点を示しています。人生の各段階で楽しみを見出し、充実した生活を送ることが可能であるというメッセージが中心です。","value":1,"parent":"1_8","density_rank_percentile":0.4375},{"level":2,"id":"2_49","label":"ポジティブな生活の追求","takeaway":"この意見は、日常生活において笑顔で過ごすことの重要性を強調しており、ポジティブな感情や幸福感を求める姿勢が表れています。人々が心地よい環境や人間関係を築き、楽しい瞬間を大切にしたいという願望が根底にあります。","value":1,"parent":"1_5","density_rank_percentile":0.453125},{"level":2,"id":"2_61","label":"失敗を受け入れる共感的なコミュニティの形成","takeaway":"この意見は、失敗を笑わずに受け入れ、互いに支え合う姿勢が重要であることを強調しています。仲間同士が理解し合い、共感を持って接することで、より良い関係を築くことができるという考えが中心です。","value":1,"parent":"1_3","density_rank_percentile":0.46875},{"level":2,"id":"2_60","label":"協力によるチームワークの重要性","takeaway":"この意見は、チームメンバーが協力し合うことの重要性を強調しています。協力することで、個々の力を結集し、より良い成果を生み出すことができるという考えが中心にあります。","value":1,"parent":"1_5","density_rank_percentile":0.484375},{"level":2,"id":"2_1","label":"他者からの好感を得るための自己表現の重要性","takeaway":"この意見は、他者から好かれるための姿勢や外見、行動が重要であるという考え方に基づいています。人間関係や社会的なつながりを築く上で、好感を持たれることが自己表現やコミュニケーションにおいて大切であるという点が強調されています。","value":1,"parent":"1_5","density_rank_percentile":0.5},{"level":2,"id":"2_54","label":"活気ある街の魅力と人々の集い","takeaway":"この意見は、賑わっている街が持つ活気や魅力について述べており、人々が集まることで生まれるコミュニティの重要性を強調しています。活気ある環境が人々を引き寄せ、交流や活動が促進される様子が表現されています。","value":1,"parent":"1_6","density_rank_percentile":0.515625},{"level":2,"id":"2_53","label":"地元愛を育む進学環境の重要性","takeaway":"この意見グループは、高校を卒業した若者が進学先で地元の良さを実感し、愛着を持てるような環境の必要性を強調しています。地元の魅力を感じられる進学先が、若者の地域への帰属意識や将来の地域貢献につながることが期待されています。","value":1,"parent":"1_8","density_rank_percentile":0.53125},{"level":2,"id":"2_44","label":"若者の誇りを育む地域づくり","takeaway":"この意見は、若者が自分の住むまちに対して誇りを持てるような環境や文化を創出することの重要性を強調しています。地域の魅力を高め、若者が自慢できる要素を取り入れることで、地域活性化や若者の定住促進につながるという視点が中心です。","value":1,"parent":"1_8","density_rank_percentile":0.546875}],"comments":{},"propertyMap":{},"translations":{},"overview":"舞鶴市では、住環境の改善や地域活性化に対する期待が高まり、市民は快適な生活環境や就職機会の増加を求めています。また、健康的で自由なライフスタイルや地域コミュニティの重要性が強調され、持続可能な社会の実現が求められています。共生と尊重に基づく平和な社会を目指し、世代を超えた活力あるまちづくりが進められています。市民の幸福を実現するため、住みやすい街づくりや地域の魅力向上が重要視されています。","config":{"name":"d3325284-0e4d-4c51-959e-241520565899","input":"d3325284-0e4d-4c51-959e-241520565899","question":"③市民として、2040年のありたい姿とは【2/15 15:00更新】","intro":"舞鶴市の地域振興に関する意見は、住民の幸福や地域の魅力向上を重視し、世代を超えた共生や持続可能な社会の構築が求められています。相互支援やポジティブな人間関係の形成、快適な生活環境の整備が強調され、特に子どもや高齢者に配慮した施策が重要視されています。また、共感や楽しさを重視し、自立した多様な生き方を支える社会の実現が期待されています。\n分析対象となったデータの件数は528件で、これらのデータに対してOpenAI API (gpt-4o-mini)を用いて99件の意見（議論）を抽出し、クラスタリングを行った。\n","model":"gpt-4o-mini","provider":"openai","is_pubcom":true,"is_embedded_at_local":false,"local_llm_address":null,"extraction":{"prompt":"あなたは専門的なリサーチアシスタントです。与えられたテキストから、意見を抽出して整理してください。\n\n# 指示\n* 入出力の例に記載したような形式で文字列のリストを返してください\n  * 必要な場合は2つの別個の意見に分割してください。多くの場合は1つの議論にまとめる方が望ましいです。\n* 整理した意見は日本語で出力してください\n\n## 入出力の例\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\"\n  ]\n}\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、市民を教育する必要がある。また、教育できる人材を養成する必要がある。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIの能力、限界、倫理的考慮事項について、市民を教育すべき\",\n    \"AIに関する教育をできる人材を養成すべき\"\n  ]\n}\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIはエネルギーグリッドを最適化して炭素排出を削減できる\"\n  ]\n}\n","workers":30,"limit":528,"properties":[],"categories":{},"category_batch_size":5,"source_code":"$19","model":"gpt-4o-mini"},"hierarchical_clustering":{"cluster_nums":[8,64],"source_code":"$1a"},"hierarchical_initial_labelling":{"prompt":"あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説し、表札（label）をつけてください。\n表札については、グループ内の具体的な論点や特徴を反映した、具体性の高い名称を考案してください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n\n# サンプルの入出力\n## 入力例\n- 手作業での意見分析は時間がかかりすぎる。AIで効率化できると嬉しい\n- 今のやり方だと分析に工数がかかりすぎるけど、AIならコストをかけずに分析できそう\n- AIが自動で意見を整理してくれると楽になって嬉しい\n\n\n## 出力例\n{\n    \"label\": \"AIによる業務効率の大幅向上とコスト効率化\",\n    \"description\": \"この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$1b","model":"gpt-4o-mini"},"hierarchical_merge_labelling":{"prompt":"あなたはデータ分析のエキスパートです。\n現在、テキストデータの階層クラスタリングを行っています。\n下層のクラスタ（意見グループ）のタイトルと説明、およびそれらのクラスタが所属する上層のクラスタのテキストのサンプルを与えるので、上層のクラスタのタイトルと説明を作成してください。\n\n# 指示\n- 統合後のクラスタ名は、統合前のクラスタ名称をそのまま引用せず、内容に基づいた新たな名称にしてください。\n- タイトルには、具体的な事象・行動（例：地域ごとの迅速対応、復興計画の着実な進展、効果的な情報共有・地域協力など）を含めてください\n  - 可能な限り具体的な表現を用いるようにし、抽象的な表現は避けてください\n    - 「多様な意見」などの抽象的な表現は避けてください\n- 出力例に示したJSON形式で出力してください\n\n\n# サンプルの入出力\n## 入力例\n- 「顧客フィードバックの自動集約」: この意見グループは、SNSやオンラインレビューなどから集めた大量の意見をAIが瞬時に解析し、企業が市場のトレンドや顧客の要望を即時に把握できる点についての期待を示しています。\n- 「AIによる業務効率の大幅向上とコスト効率化」: この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\n\n## 出力例\n{\n    \"label\": \"AI技術の導入による意見分析の効率化への期待\",\n    \"description\": \"大量の意見やフィードバックから迅速に洞察を抽出できるため、企業や自治体が消費者や市民の声を的確に把握し、戦略的な意思決定やサービス改善が可能になります。また、従来の手法と比べて作業負荷が軽減され、業務効率の向上やコスト削減といった実際の便益が得られると期待されています。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$1c","model":"gpt-4o-mini"},"hierarchical_overview":{"prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢の意見グループを分析し始めています。\nこれから意見グループのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","source_code":"$1d","model":"gpt-4o-mini"},"hierarchical_aggregation":{"sampling_num":30,"hidden_properties":{},"source_code":"$1e"},"enable_source_link":false,"output_dir":"d3325284-0e4d-4c51-959e-241520565899","skip-interaction":true,"without-html":true,"embedding":{"model":"text-embedding-3-small","source_code":"$1f"},"hierarchical_visualization":{"replacements":[],"source_code":"import subprocess\n\n\ndef hierarchical_visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../report\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"not trace of previous run"},{"step":"embedding","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_clustering","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_initial_labelling","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_merge_labelling","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_overview","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_aggregation","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_visualization","run":false,"reason":"skipping html output"}],"status":"running","start_time":"2026-02-15T04:06:54.777370","completed_jobs":[{"step":"extraction","completed":"2026-02-15T04:14:41.130197","duration":466.345257,"params":{"prompt":"あなたは専門的なリサーチアシスタントです。与えられたテキストから、意見を抽出して整理してください。\n\n# 指示\n* 入出力の例に記載したような形式で文字列のリストを返してください\n  * 必要な場合は2つの別個の意見に分割してください。多くの場合は1つの議論にまとめる方が望ましいです。\n* 整理した意見は日本語で出力してください\n\n## 入出力の例\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\"\n  ]\n}\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、市民を教育する必要がある。また、教育できる人材を養成する必要がある。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIの能力、限界、倫理的考慮事項について、市民を教育すべき\",\n    \"AIに関する教育をできる人材を養成すべき\"\n  ]\n}\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIはエネルギーグリッドを最適化して炭素排出を削減できる\"\n  ]\n}\n","workers":30,"limit":528,"properties":[],"categories":{},"category_batch_size":5,"source_code":"$20","model":"gpt-4o-mini"},"token_usage":38256},{"step":"embedding","completed":"2026-02-15T04:14:41.561271","duration":0.423456,"params":{"model":"text-embedding-3-small","source_code":"$21"},"token_usage":0},{"step":"hierarchical_clustering","completed":"2026-02-15T04:37:43.475419","duration":1381.904721,"params":{"cluster_nums":[8,64],"source_code":"$22"},"token_usage":0},{"step":"hierarchical_initial_labelling","completed":"2026-02-15T04:41:01.935804","duration":198.453923,"params":{"prompt":"あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説し、表札（label）をつけてください。\n表札については、グループ内の具体的な論点や特徴を反映した、具体性の高い名称を考案してください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n\n# サンプルの入出力\n## 入力例\n- 手作業での意見分析は時間がかかりすぎる。AIで効率化できると嬉しい\n- 今のやり方だと分析に工数がかかりすぎるけど、AIならコストをかけずに分析できそう\n- AIが自動で意見を整理してくれると楽になって嬉しい\n\n\n## 出力例\n{\n    \"label\": \"AIによる業務効率の大幅向上とコスト効率化\",\n    \"description\": \"この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$23","model":"gpt-4o-mini"},"token_usage":42586},{"step":"hierarchical_merge_labelling","completed":"2026-02-15T04:42:00.772421","duration":58.829816,"params":{"prompt":"あなたはデータ分析のエキスパートです。\n現在、テキストデータの階層クラスタリングを行っています。\n下層のクラスタ（意見グループ）のタイトルと説明、およびそれらのクラスタが所属する上層のクラスタのテキストのサンプルを与えるので、上層のクラスタのタイトルと説明を作成してください。\n\n# 指示\n- 統合後のクラスタ名は、統合前のクラスタ名称をそのまま引用せず、内容に基づいた新たな名称にしてください。\n- タイトルには、具体的な事象・行動（例：地域ごとの迅速対応、復興計画の着実な進展、効果的な情報共有・地域協力など）を含めてください\n  - 可能な限り具体的な表現を用いるようにし、抽象的な表現は避けてください\n    - 「多様な意見」などの抽象的な表現は避けてください\n- 出力例に示したJSON形式で出力してください\n\n\n# サンプルの入出力\n## 入力例\n- 「顧客フィードバックの自動集約」: この意見グループは、SNSやオンラインレビューなどから集めた大量の意見をAIが瞬時に解析し、企業が市場のトレンドや顧客の要望を即時に把握できる点についての期待を示しています。\n- 「AIによる業務効率の大幅向上とコスト効率化」: この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\n\n## 出力例\n{\n    \"label\": \"AI技術の導入による意見分析の効率化への期待\",\n    \"description\": \"大量の意見やフィードバックから迅速に洞察を抽出できるため、企業や自治体が消費者や市民の声を的確に把握し、戦略的な意思決定やサービス改善が可能になります。また、従来の手法と比べて作業負荷が軽減され、業務効率の向上やコスト削減といった実際の便益が得られると期待されています。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$24","model":"gpt-4o-mini"},"token_usage":16025},{"step":"hierarchical_overview","completed":"2026-02-15T04:42:03.580943","duration":2.797992,"params":{"prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢の意見グループを分析し始めています。\nこれから意見グループのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","source_code":"$25","model":"gpt-4o-mini"},"token_usage":1780}],"total_token_usage":98647,"token_usage_input":87473,"token_usage_output":11174,"lock_until":"2026-02-15T04:47:03.589607","current_job":"hierarchical_aggregation","current_job_started":"2026-02-15T04:42:03.589585","estimated_cost":0.01982535,"current_job_progress":null,"current_jop_tasks":null},"comment_num":528,"visibility":"public"}}],"$L26","$L27","$L28","$L29"]}],"$L2a"]
c:{"metadata":[["$","title","0",{"children":"③市民として、2040年のありたい姿とは【2/15 15:00更新】 - 「#みんなでつくる舞鶴2040」プロジェクト"}],["$","meta","1",{"name":"description","content":"舞鶴市では、住環境の改善や地域活性化に対する期待が高まり、市民は快適な生活環境や就職機会の増加を求めています。また、健康的で自由なライフスタイルや地域コミュニティの重要性が強調され、持続可能な社会の実現が求められています。共生と尊重に基づく平和な社会を目指し、世代を超えた活力あるまちづくりが進められています。市民の幸福を実現するため、住みやすい街づくりや地域の魅力向上が重要視されています。"}],["$","meta","2",{"property":"og:title","content":"③市民として、2040年のありたい姿とは【2/15 15:00更新】 - 「#みんなでつくる舞鶴2040」プロジェクト"}],["$","meta","3",{"property":"og:description","content":"舞鶴市では、住環境の改善や地域活性化に対する期待が高まり、市民は快適な生活環境や就職機会の増加を求めています。また、健康的で自由なライフスタイルや地域コミュニティの重要性が強調され、持続可能な社会の実現が求められています。共生と尊重に基づく平和な社会を目指し、世代を超えた活力あるまちづくりが進められています。市民の幸福を実現するため、住みやすい街づくりや地域の魅力向上が重要視されています。"}],["$","meta","4",{"property":"og:image","content":"http://localhost:3000/d3325284-0e4d-4c51-959e-241520565899/opengraph-image.png"}],["$","meta","5",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","6",{"name":"twitter:title","content":"③市民として、2040年のありたい姿とは【2/15 15:00更新】 - 「#みんなでつくる舞鶴2040」プロジェクト"}],["$","meta","7",{"name":"twitter:description","content":"舞鶴市では、住環境の改善や地域活性化に対する期待が高まり、市民は快適な生活環境や就職機会の増加を求めています。また、健康的で自由なライフスタイルや地域コミュニティの重要性が強調され、持続可能な社会の実現が求められています。共生と尊重に基づく平和な社会を目指し、世代を超えた活力あるまちづくりが進められています。市民の幸福を実現するため、住みやすい街づくりや地域の魅力向上が重要視されています。"}],["$","meta","8",{"name":"twitter:image","content":"http://localhost:3000/d3325284-0e4d-4c51-959e-241520565899/opengraph-image.png"}]],"error":null,"digest":"$undefined"}
11:"$c:metadata"
2b:I[49985,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],"Analysis"]
2c:I[68443,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],"Separator"]
2e:I[27787,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],"Footer"]
26:["$","$L2b",null,{"result":"$8:1:props:children:1:props:result"}]
27:["$","$L14",null,{"w":"fit-content","mx":"auto","children":["$","$L6",null,{"href":"/","children":["$","$L7",null,{"variant":"outline","size":"md","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-left","children":[["$","path","1wnfg3",{"d":"m15 18-6-6 6-6"}],"$undefined"]}],"一覧へ戻る"]}]}]}]
28:["$","$L2c",null,{"my":12,"maxW":"750px","mx":"auto"}]
29:["$","$L14",null,{"maxW":"750px","mx":"auto","mb":24,"children":"$L2d"}]
2a:["$","$L2e",null,{"meta":{"reporter":"「#みんなでつくる舞鶴2040」プロジェクト","message":"この取組では、みなさま一人ひとりの「2040年の舞鶴ってこうなってほしいな」「こんなことやってみたい！」といった想いを募集します。","webLink":"https://maizuru2040.jp/wordpress/","privacyLink":"/","termsLink":null,"brandColor":"#e7adb7","isDefault":false}}]
2f:I[24982,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],"ReporterContent"]
2d:["$","$L2f",null,{"meta":"$2a:props:meta","children":"$L30"}]
31:I[21863,["150","static/chunks/59650de3-92bcb3811df53909.js","771","static/chunks/771-6df7383af9cb22cb.js","302","static/chunks/302-413e4f1ef39e273a.js","732","static/chunks/732-1930f1f7bd6746a5.js","619","static/chunks/619-f072ac750404f9da.js","909","static/chunks/909-5e57b886fe664200.js","13","static/chunks/13-f12e2db1f59fda05.js","785","static/chunks/785-90bb983d5bf6562b.js","767","static/chunks/767-6b739569ff79f765.js","609","static/chunks/609-e6afbba7cf646b18.js","182","static/chunks/app/%5Bslug%5D/page-2d806954bbec0f0a.js"],"Image"]
30:["$","$L31",null,{"src":"/kouchouAI-reports/meta/reporter.png","alt":"「#みんなでつくる舞鶴2040」プロジェクト","maxW":"150px"}]
